{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13122429,"sourceType":"datasetVersion","datasetId":8311403}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fake or Real Text Detection: A Transformer Ensemble Model for a 91.493% Finish Score.\n\n### This notebook demonstrates how I engineered a solution for a highly data-scarce NLP problem, combining traditional ML baselines with transformer ensembles to reach a final leaderboard score of 0.91493, ranking in the top 13%\n\n## üéØGOAL : \n### The primary challenge was to distinguish between texts  from a limited training set of 95 samples and both texts in each sample - real (optimal for the recipient, as close as possible to the hidden original text) and fake (more or much more distant from the hidden original text) - have been significantly modified using LLMs.\n\n## Final Strategy\n### My final strategy was to fine-tune a *DeBERTa-large* model using a semi-supervised approach with pseudo-labels generated by a powerful 5-model transformer ensemble. This approach successfully navigated the data scarcity issue by leveraging the large, unlabeled test set, yielding a final leaderboard score of 0.91493.\n\n#### This approach was built upon the incredible work of the Kaggle community, integrating and inspired from several key ideas, like the [e.g., READX model ensemble] from the notebook by [omidbaghchehsaraei](https://www.kaggle.com/omidbaghchehsaraei) and advanced feature engineering concepts from [metawave](https://www.kaggle.com/metawave). This notebook documents how these concepts were combined and extended to create the final solution.More on Insipiration is at the end of this notebook.","metadata":{}},{"cell_type":"markdown","source":"# üßë‚Äçüç≥ Strategies: \n### After initial experiments, a key insight emerged: training a \"single-text scorer\" model ,one that learns the absolute properties of a \"real\" vs. \"fake\" text and assigns each a score, consistently outperformed direct pairwise comparison methods. This was likely due to the small dataset size, as the scorer model's training data could be doubled and perfectly debiased.\n\n## Summary of Approaches & Results\n### While I experimented with numerous techniques, here is a summary of the most effective strategies in my journey.\n\n## --- Machine Learning Only Approaches ---\n\n### - *BaseLine* (TF-IDF with logistic Regression): \nThis was my baseline/ first approach just to see with a normal and simple solution how much can a model learn. The model achieved a score of **0.778** on the leaderboard.This confirmed that while lexical features provided a strong signal, a more sophisticated approach would be required to capture the semantic and structural nuances.\n\n### -*Ensemble of Baselines*: \nThe underlying Principles are the same as the baseline but it uses predictions from 5 best machine learning models and it turned out to be logistic regression model with different Inverse Regularization (C) values. The final score from this was **0.84** this was a really good score considering how simple it is to many different models that i discussed later. *This proved lexical features alone weren‚Äôt enough.*\n\n## --- Deep-Learning Only Approach ---\n\n### - *BERT*: \nThis was the simplest model to start from , just used the tokenizer from the bert model and trained it. The result was a positive **0.83195** score. This guided to me to move towards transformers architectures as they seem to learn better patterns here then simple machine learning models.\n\n### - *Deberta with Psuedo-Labelling*: \nAfter Bert the most logical approach for me was to go for a bigger model(although i did many things in between) i.e. Deberta. Here I with the help of BERT model i did psuedo labelling of the test data and selected the labels (119) with confidence was more than 90%. This model made a score of **0.863**.*Hence Pseudo-labels gave a big boost despite limited true training data.*\n\n## --- Hybrid Approaches ---\n\n### - üîç*Forensic Expert Model*: \nHere i did several data visualization and augmentation methods to create a rich feature set. The goal here was to squeeze as much information from the 95 samples as possible to. The Results were positive it had a score of **0.82** on the leaderboard, but not quite enough to beat the ensemble of Base Lines.Here i used 'gpt2' for perplexity, 'all-MiniLM-L6-v2' for Sentence-Coherence and 'Meta-Llama-3-8B-Instruct' for LLM Judge. This model had a final score of **0.82**\n\n### - *Deberta with Baseline Ensemble (The 25-Day Champion)*: \nMy best-performing solution for over three weeks was a hybrid blend. By taking a weighted average of the probabilities from the *DeBERTa-with-Pseudo-Labeling model* and *the Ensemble of Baselines*, I achieved a score of **0.88**. This \"best of both worlds\" approach proved to be a very powerful strategy.\n\n## --- The Best Solution ---\n\n### - *R_E_A_D_X Model*: \nThe first breakthrough beyond 0.88 came from ensembling five diverse transformer architectures: **R**oBERTa, **E**LECTRA, **A**LBERT, **D**eBERTa, and **X**LNet. Training these models on the orginal 95 samples and averaging their probabilities yielded a score of **0.885**. \n\n### - üèÜ*The Champion*: \nThe final winning solution involved a form of knowledge distillation. The powerful READX ensemble was used as a \"teacher\" to generate 200 high-confidence pseudo-labels from the test set. A single DeBERTa-large model was then Pre-trained (higher learning rate) on the original data first and then fine-tuned(Lower Learning Rate) on these high-quality pseudo-labels with the original data . This \"student\" model learned the distilled knowledge of the entire ensemble, achieving the final best score of **0.91493**.\n## --- Promising but not optimal Approaches---\n\n### *DeBERTa with Domain Adaptation* :\nAn experiment was conducted to pre-train DeBERTa on the large, unlabeled test dataset before fine-tuning. This yielded a similar result to the standard fine-tuning, likely because the powerful - model adapted to the domain very quickly on its own.\n\n### *The Champion with Reinforcement Learning*:\nFrom the prediction probabilitues i had from the champion model it was obvserved that the model had many close calls where it miss labels by a 0.001 mark, By some Analysis i concluded that this happens when the texts are very similar to each other or they look very natural.A final experiment involved a custom training loop where the model was \"punished\" for being uncertain on unlabeled test data (entropy minimization). While a fascinating idea, this resulted in a slight diminishing return with a score of **0.881**, likely requiring a larger dataset to be truly effective.\n### The Below is the sole reason for trying out this RL approach with the *Chapmpion Model*. As you can see the model had close calls for many datapoints (about 130 with <10% confidence and about 70 with <1% gap) making it predict certain datapoints by a small margin. This is about 10% of the test_set of 1000+ samples.\n\n![deberta_confidence_plot.png](attachment:63e09acf-eb0c-4b2c-9a6a-ef4d7e511088.png)","metadata":{},"attachments":{"63e09acf-eb0c-4b2c-9a6a-ef4d7e511088.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"# Approach\n### The following is the approach for Both the R_E_A_D_X as well as the Deberta Large Model,as these models are interlinked, hence it was essential to include them both.\nMake Sure this [Public Dataset for Fake or Real: The imposter Hunt](https://www.kaggle.com/datasets/narayanprasaddas/public-dataset-for-fake-or-real-the-imposter-hunt) dataset is already added in the input directory,adding this to your input will make the code more seamless and easy to use, if not make sure you edit the path variables to your desired folders/paths.","metadata":{}},{"cell_type":"markdown","source":"## READX MODEL\nThis Step is not necessary to run you can jump to the final section where I processed and included the pseudo data generated from this step and just run the final step to see the results.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nTEST_DATA_PATH = '/kaggle/input/public-dataset-for-fake-or-real-the-imposter-hunt/test_data.csv'\nTRUE_TRAIN_DATA_PATH = '/kaggle/input/public-dataset-for-fake-or-real-the-imposter-hunt/train_dataset.csv'\n# PSEUDO_TRAIN_DATA_PATH = ''\ntest_df = pd.read_csv(TEST_DATA_PATH)\ntrain_df = pd.read_csv(TRUE_TRAIN_DATA_PATH)\n# pseudo_train_data = pd.read_csv(PSEUDO_TRAIN_DATA_PATH)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\naug_data = []\nfor _,row in train_df.iterrows():\n    if row['labels']==1:\n        aug_data.append({'text':row['text_1'],'label':1})\n        aug_data.append({'text':row['text_2'],'label':0})\n    else:\n        aug_data.append({'text':row['text_1'],'label':0})\n        aug_data.append({'text':row['text_2'],'label':1})\n        \naug_data = pd.DataFrame(aug_data)\naug_data['text'] = aug_data['text'].fillna('')\ntrain_df, val_df = train_test_split(aug_data, test_size=0.15, random_state=42, stratify=aug_data['label'])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import AutoTokenizer,TFAutoModelForSequenceClassification,create_optimizer\nfrom transformers import logging\nimport gc\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nlogging.set_verbosity_error()\nLR = 3e-5\nMAX_TOKEN_LEN = 512\nNUM_EPOCHS = 3\nPATIENCE = 2  \nSAVE_PATH = './best_models'\nBATCH_SIZE = 2\nSTRATEGY = tf.distribute.MirroredStrategy()\noriginal_weight = 1.0\npseudo_weight = 0.8  \nWT_DECAY = 0.01\nPRED_SAVE_PATH = './pred_proba'\nMODEL_NAMES =[\n    'google/electra-base-discriminator',\n    'albert-base-v2',\n    'microsoft/deberta-v3-base',\n    'roberta-base',\n    'distilbert-base-uncased'\n]\nSAVED_PATHS = []","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pack_data(df,tokenizer,mode='eval'):\n    print(f'Tokenizing Data...')\n    encodings = tokenizer(\n        df['text'].fillna('').tolist(),\n        truncation=True,\n        padding=True,\n        max_length=MAX_TOKEN_LEN,\n        return_tensors=\"tf\"\n    )\n    if mode == 'eval':\n        labels = df['label'].values\n        dataset = tf.data.Dataset.from_tensor_slices((\n            dict(encodings),\n            labels\n        ))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    \n    return dataset\n\ndef train_model(model_name,path):\n    patience_counter = 0\n    best_val_loss = float('inf')\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    if 'deberta-v3' in model_name:\n        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n        \n    train_dataset = pack_data(train_df,tokenizer)\n    val_dataset = pack_data(val_df,tokenizer)\n    \n    with STRATEGY.scope():\n        num_train_steps = len(train_df) * NUM_EPOCHS\n        model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n        optimizer, _ = create_optimizer(init_lr=LR, \n                                            num_warmup_steps=int(0.1 * num_train_steps), \n                                            num_train_steps=num_train_steps,\n                                            weight_decay_rate=WT_DECAY\n                                        )\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n        \n    safe_model_name = model_name.replace('/', '_')\n    final_save_path = f'{SAVE_PATH}/{safe_model_name}'\n    SAVED_PATHS.append(final_save_path)\n    \n    for epoch in range(NUM_EPOCHS):\n        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n        history = model.fit(\n            train_dataset,\n            validation_data=val_dataset,\n            epochs=1 \n        )\n        current_val_loss = history.history['val_loss'][0]\n        if current_val_loss < best_val_loss:\n            print(f\"Validation loss improved from {best_val_loss:.4f} to {current_val_loss:.4f}. Saving model.\")\n            best_val_loss = current_val_loss\n            \n            print(f\"Saving model to {final_save_path}\")\n            model.save_pretrained(final_save_path)\n            tokenizer.save_pretrained(final_save_path)\n            \n            patience_counter = 0\n        else:\n            patience_counter += 1\n            print(f\"Validation loss did not improve. Patience: {patience_counter}/{PATIENCE}\")\n    \n        if patience_counter >= PATIENCE:\n            print(\"Early stopping triggered. Training stopped.\")\n            break\n    print(\"\\n--- Training Finished ---\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for model_name in MODEL_NAMES:\n    print(f\"Starting Training of {model_name} model...\")\n    try:\n        train_model(model_name=model_name,path=SAVE_PATH)\n    except RuntimeError as e:\n        if \"out of memory\" in str(e).lower():\n           print(f\"Skipping {model_name} due to OutOfMemoryError.\")\n        else:\n            raise e\n    finally:\n        print(\"Clearing session...\")\n        tf.keras.backend.clear_session()\n        gc.collect()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_predictions = {}\n\ntest_df_text1 = test_df[['text_1']].rename(columns={'text_1': 'text'})\ntest_df_text2 = test_df[['text_2']].rename(columns={'text_2': 'text'})\n\nfor model_path in SAVED_PATHS:\n    model_name = os.path.basename(model_path)\n    print(f\"Predicting with {model_name}...\")\n    model = TFAutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    test_dataset1 = pack_data(test_df_text1, tokenizer, mode='pred')\n    logits1 = model.predict(test_dataset1).logits\n    probs1 = tf.nn.softmax(logits1)[:, 1].numpy() \n\n    test_dataset2 = pack_data(test_df_text2, tokenizer, mode='pred')\n    logits2 = model.predict(test_dataset2).logits\n    probs2 = tf.nn.softmax(logits2)[:, 1].numpy() \n\n    model_predictions[f'{model_name}_prob1'] = probs1\n    model_predictions[f'{model_name}_prob2'] = probs2\n\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n\npredictions_df = pd.DataFrame(model_predictions)\npredictions_df['id'] = test_df['id'] \nPRED_SAVE_PATH = 'Readx_ensemble_test_predictions.csv'\npredictions_df.to_csv(PRED_SAVE_PATH, index=False)\nprint(f\"\\nIndividual model predictions saved to {PRED_SAVE_PATH}\")\nprint(predictions_df.head())\n\n\nprob1_cols = [col for col in predictions_df.columns if '_prob1' in col]\nprob2_cols = [col for col in predictions_df.columns if '_prob2' in col]\n\nfinal_avg_probs_text1 = predictions_df[prob1_cols].mean(axis=1)\nfinal_avg_probs_text2 = predictions_df[prob2_cols].mean(axis=1)\n\nfinal_labels = np.where(final_avg_probs_text1 > final_avg_probs_text2, 1, 2)\ndef make_submission_csv(results):\n    df_results = pd.DataFrame(results)\n    output_df = df_results.copy()\n    output_df.columns = ['real_text_id']\n    output_df.reset_index(inplace=True)\n    output_df.rename(columns={'index': 'id'}, inplace=True)\n    output_df.to_csv('Deberta_Roberta_Electra_DisitilBERT_xlnet.csv', index=False)\n    return output_df\npred_df = make_submission_csv(final_labels) # This Yielded a score of 0.885","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Processing\nIn this section the predictions from the readx model is made into a READX_top_200.csv which inlcuded the top 200 most confident labels in the test data. You can run this Step or you can directly jump to the next section where I included it in the input dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nPSUEDO_PROB_DATA_PATH = ''\npseudo_train_data = pd.read_csv(PSUEDO_PROB_DATA_PATH)\nnew_col_names = ['id','FTroberta-base_prob1','FTroberta-base_prob2','FTdeberta-v3-base_prob1','FTdeberta-v3-base_prob2',\n                'FTelectra-base-discriminator_prob1','FTelectra-base-discriminator_prob2','FTalbert-base-v2_prob1',\n                 'FTalbert-base-v2_prob2','FTxlnet-base-cased_prob1','FTxlnet-base-cased_prob2'\n                ]\npseudo_train_data.drop(['Unnamed: 0'],axis=1,inplace=True)\ncols = ['id'] + [col for col in pseudo_train_data.columns if col != 'id']\npseudo_train_data = pseudo_train_data[cols]\npseudo_train_data.columns = new_col_names\n\nprob_1_cols = [col for col in pseudo_train_data.columns if '_prob1' in col]\nprob_2_cols = [col for col in pseudo_train_data.columns if '_prob2' in col]\n\npseudo_train_data['avg_prob1'] = pseudo_train_data[prob_1_cols].mean(axis=1)\npseudo_train_data['std_prob1'] = pseudo_train_data[prob_1_cols].std(axis=1)\npseudo_train_data['avg_prob2'] = pseudo_train_data[prob_2_cols].mean(axis=1)\npseudo_train_data['std_prob2'] = pseudo_train_data[prob_2_cols].std(axis=1)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CONFIDENCE_THRESH = 0.80\nAGREEMENT_THRESH = 0.20\nmask1  = (pseudo_train_data['avg_prob1'] > CONFIDENCE_THRESH) & (pseudo_train_data['avg_prob2'] < 1 - CONFIDENCE_THRESH)\nmask2 = (pseudo_train_data['avg_prob2'] > CONFIDENCE_THRESH) & (pseudo_train_data['avg_prob1'] < 1 - CONFIDENCE_THRESH)\nagreement_mask = (pseudo_train_data['std_prob1'] < AGREEMENT_THRESH) & (pseudo_train_data['std_prob2'] < AGREEMENT_THRESH)\nhigh_confidence_pairs = pseudo_train_data[(mask1 | mask2) & agreement_mask]\npseudo_train_data['confidence_gap'] = (pseudo_train_data['avg_prob1'] - pseudo_train_data['avg_prob2']).abs()\n\npseudo_train_data_sorted = pseudo_train_data.sort_values(by='confidence_gap', ascending=False)\npseudo_train_data_sorted\ntop_confidence_pairs = pseudo_train_data_sorted.head(NUM_PSEUDO_PAIRS)\n\ntop_confidence_pairs['id'] = top_confidence_pairs['id'].astype('int64') \n\nprint(top_confidence_pairs)\npsuedo_label_dataset = pd.merge(test_df,\n                                top_confidence_pairs,\n                               left_on='id',\n                               right_on='id',\n                               how='inner')\n\npsuedo_label_dataset\npsuedo_label_dataset['label'] = np.where(psuedo_label_dataset['avg_prob1'] > psuedo_label_dataset['avg_prob2'], 1, 2)\npsuedo_label_dataset = psuedo_label_dataset[['id', 'text_1', 'text_2', 'label']]\npsuedo_label_dataset.to_csv('READX_top_200.csv')\nprint(\"Final Pseudo-Labeled DataFrame and made READX_top_200.csv :\")\npsuedo_label_dataset","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üèÜ The Champion\nThis Part includes the pseudo labelled data from the Readx model, You can use it directly from the input dataset i provided or you can run those 5 models first and then use it.\n\n> **‚ö†Ô∏èNOTE**: Before Running this step If you have run and trained the previous models make sure to *Restart and Clear Cell Outputs* to avoid any potential Memory Overflow issues.\n\n> Also this Section is broadly divided into 4 parts, i would again highly recommend to run the pre-training , fine-tuning and test step seprately to avoid memory overflow,each time you use *Restart and Clear Cell Outputs* make sure to **re-run Data Augementation and Functions** part to avoid erros.","metadata":{}},{"cell_type":"markdown","source":"## Data Augementation and Functions\n","metadata":{}},{"cell_type":"code","source":"# Hyper Parameters\nimport tensorflow as tf\nfrom transformers import AutoTokenizer,TFAutoModelForSequenceClassification,create_optimizer,AutoConfig\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nLR = 1.5e-5\nMAX_TOKEN_LEN = 256\nNUM_EPOCHS = 2\nPATIENCE = 2  \nBATCH_SIZE = 1\nORIGINAL_WEIGHT = 1.0\nPSEUDO_WEIGHT = 0.6\nWT_DECAY = 0.1\nCONFIDENCE_THRESH = 0.85 \nAGREEMENT_THRESH = 0.10\nNUM_PSEUDO_PAIRS = 200\nSTRATEGY = tf.distribute.MirroredStrategy()\nTEST_DATA_PATH = '/kaggle/input/public-dataset-for-fake-or-real-the-imposter-hunt/test_data.csv'\nTRUE_TRAIN_DATA_PATH = '/kaggle/input/public-dataset-for-fake-or-real-the-imposter-hunt/train_dataset.csv'\nPSEUDO_TRAIN_DATA_PATH = '/kaggle/input/public-dataset-for-fake-or-real-the-imposter-hunt/READX_top_200.csv'\nMODEL_NAME = 'microsoft/deberta-v3-large'\nSAVE_PATH = f'./best_model'\nPRED_SAVE_PATH = './pred_proba'\nSAVED_PATHS = []","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment_data(df):\n    if 'label' in df.columns:\n        df['labels'] = df['label']\n    real_texts = pd.concat([\n        df.loc[df['labels'] == 1, 'text_1'],\n        df.loc[df['labels'] == 2, 'text_2']\n    ])\n    fake_texts = pd.concat([\n        df.loc[df['labels'] == 1, 'text_2'],\n        df.loc[df['labels'] == 2, 'text_1']\n    ])\n    df_real = pd.DataFrame({'text': real_texts, 'label': 1})\n    df_fake = pd.DataFrame({'text': fake_texts, 'label': 0})\n    aug_data = pd.concat([df_real, df_fake], ignore_index=True)\n    aug_data = aug_data.sample(frac=1).reset_index(drop=True)\n    return aug_data\ntest_df = pd.read_csv(TEST_DATA_PATH)\ntrain_df = pd.read_csv(TRUE_TRAIN_DATA_PATH)\npseudo_train_data = pd.read_csv(PSEUDO_TRAIN_DATA_PATH)\n\ntrain_df_, val_df = train_test_split(train_df, \n                                     test_size=0.30, \n                                     random_state=42, \n                                     stratify=train_df['labels'])\naug_train_df = augment_data(train_df_)\nval_df = augment_data(val_df)\naug_psuedo_train_df = augment_data(pseudo_train_data)\naug_train_df['source'] = 'original'\naug_psuedo_train_df['source'] = 'pseudo'\njoint_train_df = pd.concat([aug_train_df,aug_psuedo_train_df],ignore_index=True)\njoint_train_df['sample_weight'] = np.where(\n    joint_train_df['source'] == 'original', \n    ORIGINAL_WEIGHT,      \n    PSEUDO_WEIGHT     \n)\njoint_train_df = joint_train_df.sample(frac=1).reset_index(drop=True)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pack_data(df,tokenizer,mode='train'):\n    print(f'Tokenizing Data...')\n    cols = df.columns\n    encodings = tokenizer(\n        df['text'].fillna('').tolist(),\n        truncation=True,\n        padding=True,\n        max_length=MAX_TOKEN_LEN,\n        return_tensors=\"tf\"\n    )\n    if 'sample_weight' in cols:\n        labels = df['label'].values\n        weights = df['sample_weight'].values\n        dataset = tf.data.Dataset.from_tensor_slices((\n            dict(encodings),\n            labels,\n            weights\n        ))\n    elif 'labels' or 'label' in cols:\n        labels = df['label'].values\n        dataset = tf.data.Dataset.from_tensor_slices((\n            dict(encodings),\n            labels,\n        ))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    return dataset\nfrom transformers import logging\nimport gc\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nlogging.set_verbosity_error()\ndef train_model(model_name,\n                train_df,\n                val_df,\n                path,\n                config):\n    patience_counter = 0\n    best_val_loss = float('inf')\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    if 'deberta-v3' in model_name:\n        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n        \n    train_dataset = pack_data(train_df,tokenizer,mode='train')\n    val_dataset = pack_data(val_df,tokenizer,mode='val')\n    \n    with STRATEGY.scope():\n        num_train_steps = len(train_df) * NUM_EPOCHS\n        model = TFAutoModelForSequenceClassification.from_pretrained(model_name,\n                                                                     config=config)\n        optimizer, _ = create_optimizer(init_lr=LR, \n                                            num_warmup_steps=int(0.1 * num_train_steps), \n                                            num_train_steps=num_train_steps,\n                                            weight_decay_rate=WT_DECAY\n                                        )\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n        \n    safe_model_name = model_name.replace('/', '_')\n    final_save_path = f'{SAVE_PATH}/{safe_model_name}'\n    SAVED_PATHS.append(final_save_path)\n    \n    for epoch in range(NUM_EPOCHS):\n        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n        history = model.fit(\n            train_dataset,\n            validation_data=val_dataset,\n            epochs=1 \n        )\n        current_val_loss = history.history['val_loss'][0]\n        if current_val_loss < best_val_loss:\n            print(f\"Validation loss improved from {best_val_loss:.4f} to {current_val_loss:.4f}. Saving model.\")\n            best_val_loss = current_val_loss\n            \n            print(f\"Saving model to {final_save_path}\")\n            model.save_pretrained(final_save_path)\n            tokenizer.save_pretrained(final_save_path)\n            \n            patience_counter = 0\n        else:\n            patience_counter += 1\n            print(f\"Validation loss did not improve. Patience: {patience_counter}/{PATIENCE}\")\n    \n        if patience_counter >= PATIENCE:\n            print(\"Early stopping triggered. Training stopped.\")\n            break\n    print(\"\\n--- Training Finished ---\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pre-Training Step","metadata":{}},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(MODEL_NAME)\nconfig.classifier_dropout = 0.1 \nconfig.num_labels = 2\ntrain_model(model_name=MODEL_NAME,\n            train_df=aug_train_df,\n            val_df=val_df,\n            path=SAVE_PATH,\n            config=config\n           )","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-09-21T04:39:54.292Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine-Tuning-Step","metadata":{}},{"cell_type":"code","source":"SAVED_PATHS = ['./best_models/microsoft_deberta-v3-large']\n\nSAVE_PATH = SAVE_PATH+'_fine_tuned'\nLR = 5e-6\nconfig = AutoConfig.from_pretrained(SAVED_PATHS[0])\nconfig.classifier_dropout = 0.1 \nconfig.num_labels = 2\ntrain_model(\n            model_name=SAVED_PATHS[0],\n            train_df=joint_train_df,\n            val_df=val_df,\n            path=SAVE_PATH,\n            config=config\n            )","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test-Step","metadata":{}},{"cell_type":"code","source":"CURR_SAVE_PATHS = ['./best_models_fine_tuned_fine_tuned/._best_models_microsoft_deberta-v3-large']\ntest_df_text1 = test_df[['text_1']].rename(columns={'text_1': 'text'})\ntest_df_text2 = test_df[['text_2']].rename(columns={'text_2': 'text'})\nmodel_predictions = {}\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nfor model_path in CURR_SAVE_PATHS:\n    print(f'---------------predicting with saved model{model_path}---------------')\n    print('---------------Loading Model and Tokenizer---------------')\n    with STRATEGY.scope():\n        model = TFAutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    print('---------------Tokenizing and Predicting on Text1---------------')\n    \n    test_dataset1 = pack_data(test_df_text1, tokenizer, mode='pred')\n    logits1 = model.predict(test_dataset1).logits\n    probs1 = tf.nn.softmax(logits1)[:, 1].numpy() \n    \n    print('---------------Tokenizing and Predicting on Text2---------------')\n    test_dataset2 = pack_data(test_df_text2, tokenizer, mode='pred')\n    logits2 = model.predict(test_dataset2).logits\n    probs2 = tf.nn.softmax(logits2)[:, 1].numpy() \n\n    model_predictions[f'{model_path}_prob1'] = probs1\n    model_predictions[f'{model_path}_prob2'] = probs2\n    print('---------------Finished Model Predictions---------------')\n    tf.keras.backend.clear_session()\n    gc.collect()\npredictions_df = pd.DataFrame(model_predictions)\npredictions_df['id'] = test_df['id'] \n# PRED_SAVE_PATH = 'deberta(0.91493)_large_test_predictions.csv'\npredictions_df.to_csv(PRED_SAVE_PATH, index=False)\nprint(f\"\\nIndividual model predictions saved to {PRED_SAVE_PATH}\")\nprint(predictions_df.head())\n\nprob1_cols = [col for col in predictions_df.columns if '_prob1' in col]\nprob2_cols = [col for col in predictions_df.columns if '_prob2' in col]\n\nfinal_avg_probs_text1 = predictions_df[prob1_cols].mean(axis=1)\nfinal_avg_probs_text2 = predictions_df[prob2_cols].mean(axis=1)\n\nfinal_labels = np.where(final_avg_probs_text1 > final_avg_probs_text2, 1, 2)\ndef make_submission_csv(results,name=None):\n    df_results = pd.DataFrame(results)\n    output_df = df_results.copy()\n    output_df.columns = ['real_text_id']\n    output_df.reset_index(inplace=True)\n    output_df.rename(columns={'index': 'id'}, inplace=True)\n    if name!=None:\n        output_df.to_csv(name, index=False)\n    return output_df\npred_df = make_submission_csv(final_labels,name='Deberta_large_on_READX_200.csv')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Learnings \n\n### There is a lot that i learned in this journey, in the beginning i was able to build ML models but this Competition inspired me to learn a lot of NLP. From Various Data Augmentation techniques to finetuning a large model , i Learned a lot, and here are the few key things i learned:\n\n- Baselines Matter: Simple models like TF-IDF + Logistic Regression gave surprisingly strong starting points.\n- Scarcity Hacks: With only 95 samples, creative techniques (pseudo-labelling, augmentation) were essential.\n- Pseudo-Labels : Using confident test predictions as training data boosted performance significantly.\n- Data-spliting : Learned not only about train-test-split but also about groupkfolds, strategies in splitting data with key values.\n- Power of Ensembling /Stacking : Combining multiple models captured diverse patterns better than any single model, also may a times stacking models seems to work better than ensembles.\n- Hybrid Wins : Blending ML features with transformer predictions outperformed either alone.\n- Progressive Scaling : Moving from small (BERT) to larger (DeBERTa-Large) models gave incremental gains.\n- Confidence Filtering : Selecting only high-confidence pseudo-labels avoided noise and improved stability.\n- Custom Loss Insight : Tried reinforcement-style loss adjustments‚Äîshowed potential but limited by data.\n- Interpretability Attempts : Features like perplexity and coherence added human-style checks, though not always better than pure models.\n\n- Iteration Counts : Improvement came step by step; no single ‚Äúmagic trick,‚Äù but layering ideas carefully.\n### This Competition was an intense sprint for me, completed in just under 4 weeks after joining the competition late. While there are always more avenues to explore, I am satisfied of the robust and high-performing pipeline that I developed in this limited timeframe and looking forward to applying these learnings to future challenges and projects.","metadata":{}},{"cell_type":"markdown","source":"# Acknowledgements & Inspirations\n\n### This journey would not have been possible without the amazing Kaggle community. I learned a great deal from public notebooks, studying how other data scientists approach and solve complex problems with clever and effective techniques. My sincere thanks to the entire community and to the organizers for hosting this fantastic competition.\n### In particular, I'd like to acknowledge the following inspirations:\n- My perplexity and LLM judge idea is inspired from [This Notebook](https://www.kaggle.com/code/metawave/memory-optimized-transformers-for-impostor-hunt)\n- MY READX model was an direct inspiration from [This Notebook](https://www.kaggle.com/code/omidbaghchehsaraei/averaging-ensembling-for-the-impostor-hunt-in-text/notebook)","metadata":{}}]}