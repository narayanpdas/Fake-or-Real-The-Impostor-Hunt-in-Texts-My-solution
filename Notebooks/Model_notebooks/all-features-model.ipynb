{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":105874,"databundleVersionId":12964783,"sourceType":"competition"},{"sourceId":12970803,"sourceType":"datasetVersion","datasetId":8100028},{"sourceId":13005419,"sourceType":"datasetVersion","datasetId":8028993}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\ntrain_df = pd.read_csv('/kaggle/input/fake-or-real-the-imposter-x-train/train_dataset.csv')\ntest_df = pd.read_csv('/kaggle/input/fake-or-real-the-imposter-x-train/test_data.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:24:20.228394Z","iopub.execute_input":"2025-09-09T14:24:20.228625Z","iopub.status.idle":"2025-09-09T14:24:20.683758Z","shell.execute_reply.started":"2025-09-09T14:24:20.228607Z","shell.execute_reply":"2025-09-09T14:24:20.682619Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def preprocess_text(text: str, max_lines: int = 45) -> str:\n    \"\"\"\n    Preprocesses a text by trimming it to a max number of lines\n    and cleaning it to keep only English letters, numbers, and basic punctuation.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # 1. Limit the article to a maximum of 45-50 lines\n    lines = text.splitlines()\n    trimmed_lines = lines[:max_lines]\n    trimmed_text = \"\\n\".join(trimmed_lines)\n\n    # 2. Keep only English letters, numbers, and specified marks\n    # This regex pattern finds any character that is NOT a-z, A-Z, 0-9,\n    # a whitespace character, or one of ? . ! ,\n    # and replaces it with a space.\n    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s?.!,]', ' ', trimmed_text)\n    \n    # Optional: Clean up extra whitespace created by the substitution\n    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n\n    return cleaned_text\ntrain_df['text_1'] = train_df['text_1'].apply(preprocess_text)\ntrain_df['text_2'] = train_df['text_2'].apply(preprocess_text)\ntest_df['text_1'] = test_df['text_1'].apply(preprocess_text)\ntest_df['text_2'] = test_df['text_2'].apply(preprocess_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T05:14:16.257477Z","iopub.execute_input":"2025-09-08T05:14:16.257798Z","iopub.status.idle":"2025-09-08T05:14:16.635796Z","shell.execute_reply.started":"2025-09-08T05:14:16.257776Z","shell.execute_reply":"2025-09-08T05:14:16.634798Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('brown')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:24:53.445138Z","iopub.execute_input":"2025-09-09T14:24:53.445429Z","iopub.status.idle":"2025-09-09T14:24:55.245466Z","shell.execute_reply.started":"2025-09-09T14:24:53.445409Z","shell.execute_reply":"2025-09-09T14:24:55.244402Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package brown to /usr/share/nltk_data...\n[nltk_data]   Package brown is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"text1 = train_df['text_1'].to_list()\ntext2 = train_df['text_2'].to_list()\ntest_text1 = test_df['text_1'].to_list()\ntest_text2 = test_df['text_2'].to_list()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:24:56.653217Z","iopub.execute_input":"2025-09-09T14:24:56.653634Z","iopub.status.idle":"2025-09-09T14:24:56.663997Z","shell.execute_reply.started":"2025-09-09T14:24:56.653614Z","shell.execute_reply":"2025-09-09T14:24:56.663159Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import math\nfrom collections import Counter\n\ndef calculate_shannon_entropy(text: str) -> float:\n    \"\"\"\n    Calculates the Shannon entropy of a text at the character level.\n    Higher values indicate more randomness or complexity.\n    \"\"\"\n    if not text:\n        return 0.0\n\n    char_counts = Counter(text)\n    total_chars = len(text)\n\n    entropy = 0.0\n    for count in char_counts.values():\n        probability = count / total_chars\n        entropy -= probability * math.log2(probability)\n        \n    return entropy\n\nentropy_train_text = []\nentropy_test_text = []\ntrain_df['text_1'].fillna(' ',inplace=True)\ntrain_df['text_2'].fillna(' ',inplace=True)\nfor _,row in train_df.iterrows():\n    ent_1 = calculate_shannon_entropy(row['text_1'])\n    entropy_train_text.append(ent_1)\nfor _,row in train_df.iterrows():\n    ent_2 = calculate_shannon_entropy(row['text_2'])\n    entropy_train_text.append(ent_2)\n    \n\ntest_df['text_1'].fillna('',inplace=True)\ntest_df['text_2'].fillna('',inplace=True)\n\nfor _,row in test_df.iterrows():\n    ent_1 = calculate_shannon_entropy(row['text_1'])\n    entropy_test_text.append(ent_1)\nfor _,row in test_df.iterrows():\n    ent_2 = calculate_shannon_entropy(row['text_2'])\n    entropy_test_text.append(ent_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:25:09.292278Z","iopub.execute_input":"2025-09-09T14:25:09.292548Z","iopub.status.idle":"2025-09-09T14:25:09.642187Z","shell.execute_reply.started":"2025-09-09T14:25:09.292529Z","shell.execute_reply":"2025-09-09T14:25:09.641195Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2719569783.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df['text_1'].fillna(' ',inplace=True)\n/tmp/ipykernel_36/2719569783.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df['text_2'].fillna(' ',inplace=True)\n/tmp/ipykernel_36/2719569783.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  test_df['text_1'].fillna('',inplace=True)\n/tmp/ipykernel_36/2719569783.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  test_df['text_2'].fillna('',inplace=True)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"len(entropy_test_text) , len(entropy_train_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:25:14.798243Z","iopub.execute_input":"2025-09-09T14:25:14.798497Z","iopub.status.idle":"2025-09-09T14:25:14.804831Z","shell.execute_reply.started":"2025-09-09T14:25:14.798479Z","shell.execute_reply":"2025-09-09T14:25:14.803928Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(2136, 190)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"!pip install pandas spacy textstat pyspellchecker\n!python -m spacy download en_core_web_sm\n!pip install textstat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:25:17.918742Z","iopub.execute_input":"2025-09-09T14:25:17.918996Z","iopub.status.idle":"2025-09-09T14:25:40.107919Z","shell.execute_reply.started":"2025-09-09T14:25:17.918978Z","shell.execute_reply":"2025-09-09T14:25:40.106926Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\nCollecting textstat\n  Downloading textstat-0.7.10-py3-none-any.whl.metadata (15 kB)\nCollecting pyspellchecker\n  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.4)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\nCollecting pyphen (from textstat)\n  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from textstat) (3.9.1)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\nRequirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\nCollecting numpy>=1.23.2 (from pandas)\n  Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->textstat) (1.5.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->textstat) (2024.11.6)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\nINFO: pip is looking at multiple versions of mkl-fft to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_fft (from numpy>=1.23.2->pandas)\n  Downloading mkl_fft-2.0.0-22-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.1 kB)\nCollecting numpy>=1.23.2 (from pandas)\n  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hINFO: pip is looking at multiple versions of mkl-random to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_random (from numpy>=1.23.2->pandas)\n  Downloading mkl_random-1.2.11-22-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nINFO: pip is looking at multiple versions of mkl-umath to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_umath (from numpy>=1.23.2->pandas)\n  Downloading mkl_umath-0.2.0-21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nDownloading textstat-0.7.10-py3-none-any.whl (239 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyspellchecker, pyphen, numpy, textstat\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\nydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.2.6 pyphen-0.17.2 pyspellchecker-0.8.3 textstat-0.7.10\nCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\nRequirement already satisfied: textstat in /usr/local/lib/python3.11/dist-packages (0.7.10)\nRequirement already satisfied: pyphen in /usr/local/lib/python3.11/dist-packages (from textstat) (0.17.2)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from textstat) (3.9.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.2.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->textstat) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->textstat) (1.5.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->textstat) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->textstat) (4.67.1)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport spacy\nimport textstat\nfrom spellchecker import SpellChecker\nimport re\n\nnlp = spacy.load(\"en_core_web_sm\")\nspell = SpellChecker()\n\ndef extract_all_features(sentence_list):\n    \"\"\"\n    Processes a list of sentences and extracts a comprehensive set of features.\n    \n    Args:\n        sentence_list (list): A list of strings, where each string is a text to analyze.\n        \n    Returns:\n        pandas.DataFrame: A DataFrame with the extracted features for each sentence.\n    \"\"\"\n    feature_names = [\n        'text', 'flesch_reading_ease', 'flesch_kincaid_grade', \n        'exclamation_count', 'misspelled_word_count', \n        'vocabulary_richness_ttr', 'named_entity_count', 'proper_noun_count'\n    ]\n    all_features = []\n\n    for sentence in sentence_list:\n        if not isinstance(sentence, str):\n            default_features = {name: 0 for name in feature_names}\n            default_features['text'] = str(sentence) \n            all_features.append(default_features)\n            continue \n        doc = nlp(sentence)\n\n        reading_ease = textstat.flesch_reading_ease(sentence)\n        grade_level = textstat.flesch_kincaid_grade(sentence)\n\n        exclamation_count = sentence.count('!')\n        \n        clean_tokens = [token.text.lower() for token in doc if token.is_alpha]\n        misspelled_count = len(spell.unknown(clean_tokens))\n        if len(clean_tokens) > 0:\n            ttr = len(set(clean_tokens)) / len(clean_tokens)\n        else:\n            ttr = 0\n            \n        entity_count = len(doc.ents)\n        proper_noun_count = sum(1 for token in doc if token.pos_ == 'PROPN')\n\n        features = {\n            'text': sentence,\n            'flesch_reading_ease': reading_ease,\n            'flesch_kincaid_grade': grade_level,\n            'exclamation_count': exclamation_count,\n            'misspelled_word_count': misspelled_count,\n            'vocabulary_richness_ttr': ttr,\n            'named_entity_count': entity_count,\n            'proper_noun_count': proper_noun_count,\n        }\n        all_features.append(features)\n    return pd.DataFrame(all_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:25:48.714828Z","iopub.execute_input":"2025-09-09T14:25:48.715204Z","iopub.status.idle":"2025-09-09T14:25:54.019398Z","shell.execute_reply.started":"2025-09-09T14:25:48.715170Z","shell.execute_reply":"2025-09-09T14:25:54.018638Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"feature_df1 = extract_all_features(text1)\nfeature_df2 = extract_all_features(text2)\nfeature_df_test_1 = extract_all_features(test_text1)\nfeature_df_test_2 = extract_all_features(test_text2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:26:09.561238Z","iopub.execute_input":"2025-09-09T14:26:09.561504Z","iopub.status.idle":"2025-09-09T14:28:24.724522Z","shell.execute_reply.started":"2025-09-09T14:26:09.561487Z","shell.execute_reply":"2025-09-09T14:28:24.723720Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"perplexity_score_train = pd.read_csv('/kaggle/input/fake-or-real-the-imposter-x-train/perplexity_score_features_train.csv')\nperplexity_score_test = pd.read_csv('/kaggle/input/fake-or-real-the-imposter-x-train/perplexity_score_features_test.csv')\nllm_judge_train = pd.read_csv('/kaggle/input/fake-or-real-the-imposter-x-train/llm_judge_feature_train_individual.csv')\nllm_judge_test = pd.read_csv('/kaggle/input/fake-or-real-the-imposter-x-train/llm_judge_feature_test_individual_.csv')\nsemantic_focus_train = np.load('/kaggle/input/fake-or-real-the-imposter-x-train/semantic_focus_train.npy')\nsemantic_focus_test = np.load('/kaggle/input/fake-or-real-the-imposter-x-train/semantic_focus_test.npy')\nflow_coherence_train = np.load('/kaggle/input/fake-or-real-the-imposter-x-train/flow_coherence_train.npy')\nflow_coherence_test = np.load('/kaggle/input/fake-or-real-the-imposter-x-train/flow_coherence_test.npy')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:28:54.413986Z","iopub.execute_input":"2025-09-09T14:28:54.414263Z","iopub.status.idle":"2025-09-09T14:28:54.456472Z","shell.execute_reply.started":"2025-09-09T14:28:54.414245Z","shell.execute_reply":"2025-09-09T14:28:54.455386Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"features_text1 = pd.DataFrame({\n    'proper_noun_count': feature_df1['proper_noun_count'],\n    'exclamation_count': feature_df1['exclamation_count'],\n    'flesch_kincaid_grade': feature_df1['flesch_kincaid_grade'],\n    'flesch_reading_ease':feature_df1['flesch_reading_ease'],\n    'misspelled_word_count':feature_df1['misspelled_word_count'],\n    'vocabulary_richness_ttr':feature_df1['vocabulary_richness_ttr']\n})\nfeatures_text2 = pd.DataFrame({\n    'proper_noun_count': feature_df2['proper_noun_count'],\n    'exclamation_count': feature_df2['exclamation_count'],\n    'flesch_kincaid_grade':feature_df2['flesch_kincaid_grade'],\n    'flesch_reading_ease':feature_df2['flesch_reading_ease'],\n    'misspelled_word_count':feature_df2['misspelled_word_count'],\n    'vocabulary_richness_ttr':feature_df2['vocabulary_richness_ttr']\n})\nfeatures_test_text1 = pd.DataFrame({\n    'proper_noun_count': feature_df_test_1['proper_noun_count'],\n    'exclamation_count': feature_df_test_1['exclamation_count'],\n    'flesch_kincaid_grade': feature_df_test_1['flesch_kincaid_grade'],\n    'flesch_reading_ease':feature_df_test_1['flesch_reading_ease'],\n    'misspelled_word_count':feature_df_test_1['misspelled_word_count'],\n    'vocabulary_richness_ttr':feature_df_test_1['vocabulary_richness_ttr']\n})\nfeatures_test_text2 = pd.DataFrame({\n    'proper_noun_count': feature_df_test_2['proper_noun_count'],\n    'exclamation_count': feature_df_test_2['exclamation_count'],\n    'flesch_kincaid_grade': feature_df_test_2['flesch_kincaid_grade'],\n    'flesch_reading_ease':feature_df_test_2['flesch_reading_ease'],\n    'misspelled_word_count':feature_df_test_2['misspelled_word_count'],\n    'vocabulary_richness_ttr':feature_df_test_2['vocabulary_richness_ttr']\n})\nfeature_df_train = pd.concat([features_text1,features_text2])\nfeature_df_test = pd.concat([features_test_text1,features_test_text2])\nfeature_df_train['target'] = np.concatenate([np.where(train_df['labels'] == 1, 1, 0),np.where(train_df['labels'] == 1, 0, 1)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:30:39.701928Z","iopub.execute_input":"2025-09-09T14:30:39.702246Z","iopub.status.idle":"2025-09-09T14:30:39.716127Z","shell.execute_reply.started":"2025-09-09T14:30:39.702225Z","shell.execute_reply":"2025-09-09T14:30:39.715166Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"x_train_df = feature_df_train.copy()\nx_final_test = feature_df_test.copy()\nx_train_df = x_train_df.drop(['target'],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:31:51.187531Z","iopub.execute_input":"2025-09-09T14:31:51.188303Z","iopub.status.idle":"2025-09-09T14:31:51.195933Z","shell.execute_reply.started":"2025-09-09T14:31:51.188267Z","shell.execute_reply":"2025-09-09T14:31:51.194924Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"x_train_df['perplexity_score'] = perplexity_score_train['perplexity_score']\nx_train_df['llm_judge'] = llm_judge_train['llm_judge_verdict']\nx_train_df['semantic_focus'] = semantic_focus_train\nx_train_df['flow_coherence'] = flow_coherence_train\nx_train_df['entropy_diff'] = entropy_train_text\n\nx_final_test['perplexity_score'] = perplexity_score_test['perplexity_score']\nx_final_test['llm_judge'] = llm_judge_test['llm_judge_verdict']\nx_final_test['semantic_focus'] = semantic_focus_test\nx_final_test['flow_coherence'] = flow_coherence_test\nx_final_test['entropy_diff'] = entropy_test_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:31:53.426646Z","iopub.execute_input":"2025-09-09T14:31:53.427100Z","iopub.status.idle":"2025-09-09T14:31:53.441023Z","shell.execute_reply.started":"2025-09-09T14:31:53.427069Z","shell.execute_reply":"2025-09-09T14:31:53.440298Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"x_final_test['perplexity_score'] = x_final_test['perplexity_score'].fillna(x_final_test['perplexity_score'].mean())\nx_final_test['llm_judge'] = pd.to_numeric(x_final_test['llm_judge'],errors='coerce')\nx_final_test['llm_judge'] = x_final_test['llm_judge'].fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:45:44.696610Z","iopub.execute_input":"2025-09-09T14:45:44.696965Z","iopub.status.idle":"2025-09-09T14:45:44.707449Z","shell.execute_reply.started":"2025-09-09T14:45:44.696936Z","shell.execute_reply":"2025-09-09T14:45:44.705853Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"x_train_df['perplexity_score'] = x_train_df['perplexity_score'].fillna(x_train_df['perplexity_score'].mean())\nx_train_df['llm_judge'] = pd.to_numeric(x_train_df['llm_judge'],errors='coerce')\nx_train_df['llm_judge'] = x_train_df['llm_judge'].fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:45:01.458598Z","iopub.execute_input":"2025-09-09T14:45:01.459010Z","iopub.status.idle":"2025-09-09T14:45:01.465420Z","shell.execute_reply.started":"2025-09-09T14:45:01.458982Z","shell.execute_reply":"2025-09-09T14:45:01.464069Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"selected_features = ['proper_noun_count','exclamation_count','flesch_kincaid_grade','flesch_reading_ease','perplexity_score',\n                    'llm_judge','semantic_focus','flow_coherence','entropy_diff']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:31:56.429012Z","iopub.execute_input":"2025-09-09T14:31:56.429323Z","iopub.status.idle":"2025-09-09T14:31:56.434375Z","shell.execute_reply.started":"2025-09-09T14:31:56.429302Z","shell.execute_reply":"2025-09-09T14:31:56.432882Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nsns.set_style(\"whitegrid\")\nfeatures_to_plot = ['proper_noun_count', 'exclamation_count', 'flesch_kincaid_grade',\n                    'flesch_reading_ease','perplexity_score' , 'llm_judge','semantic_focus','flow_coherence',\n                    'entropy_diff'\n                   ]\nn_features = len(features_to_plot)\nncols = 3 \nnrows = math.ceil(n_features / ncols)\nfig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 5 * nrows))\n\nfig.suptitle('Distribution of Feature Differences by Class', fontsize=20)\nfor i, feature in enumerate(features_to_plot):\n    ax = axes.flatten()[i] # Select the correct subplot\n    sns.histplot(data=x_train_df, x=feature, hue='target', kde=True, ax=ax, palette='viridis')\n    ax.set_title(f'Difference in \"{feature}\"')\n    \n    if x_train_df[feature].nunique() > 2:\n      ax.axvline(x=0, color='red', linestyle='--', linewidth=1.5)\n    \n    # Clean up legends\n    if i == 0:\n      ax.legend(title='Is Text 1 Real?')\n    else:\n      if ax.get_legend() is not None:\n        ax.get_legend().remove()\n\n# --- Hide any unused subplots in the last row ---\nfor i in range(n_features, len(axes.flatten())):\n    axes.flatten()[i].set_visible(False)\n\nplt.tight_layout(rect=[0, 0, 1, 0.97]) # Adjust for suptitle\nplt.show()\nplt.savefig('all_feature_distribution_plot.png')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train_df = np.concatenate([np.where(train_df['labels'] == 1, 1, 0),np.where(train_df['labels'] == 1, 0, 1)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:32:44.964486Z","iopub.execute_input":"2025-09-09T14:32:44.964860Z","iopub.status.idle":"2025-09-09T14:32:44.973266Z","shell.execute_reply.started":"2025-09-09T14:32:44.964835Z","shell.execute_reply":"2025-09-09T14:32:44.971450Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x_train_df,\n                                                 y_train_df,\n                                                 random_state = 42,\n                                                 test_size=0.15,\n                                                 shuffle=True,\n                                                 stratify=y_train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:45:52.676634Z","iopub.execute_input":"2025-09-09T14:45:52.677440Z","iopub.status.idle":"2025-09-09T14:45:52.687015Z","shell.execute_reply.started":"2025-09-09T14:45:52.677403Z","shell.execute_reply":"2025-09-09T14:45:52.685977Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nparameters = {\n    'classifier': [LogisticRegression(solver='liblinear', class_weight='balanced')],\n    'classifier__C': [0.1, 1, 10, 100],\n}\npipeline = Pipeline([\n            ('classifier',LogisticRegression())\n    ])\nrandom_search = RandomizedSearchCV(\n        pipeline,\n        param_distributions=parameters,\n        n_iter=30, \n        cv=5,    \n        scoring='accuracy',\n        verbose=1,\n        random_state=42\n    )\nrandom_search.fit(x_train.to_numpy(), y_train)\nprint('Completed search...')\nprint(f\"Best hyperparameters: {random_search.best_params_}\")\nprint(f\"Best cross-validation score: {random_search.best_score_:.4f}\")\nbest_model = random_search.best_estimator_\nbest_model.score(x_test, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:45:56.561428Z","iopub.execute_input":"2025-09-09T14:45:56.561730Z","iopub.status.idle":"2025-09-09T14:45:56.645690Z","shell.execute_reply.started":"2025-09-09T14:45:56.561710Z","shell.execute_reply":"2025-09-09T14:45:56.644846Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 4 candidates, totalling 20 fits\nCompleted search...\nBest hyperparameters: {'classifier__C': 0.1, 'classifier': LogisticRegression(C=0.1, class_weight='balanced', solver='liblinear')}\nBest cross-validation score: 0.8072\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 4 is smaller than n_iter=30. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n  warnings.warn(\n","output_type":"stream"},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"0.7241379310344828"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparameters = {\n    'classifier': [LogisticRegression(solver='liblinear', class_weight='balanced')],\n    'classifier__C': [0.1, 1, 10, 100],\n}\ngrid_search = GridSearchCV(\n    estimator=pipeline,\n    param_grid=parameters,\n    scoring='accuracy',\n    cv=5\n)\ngrid_search.fit(x_train, y_train)\nprint(f'completed search...')\nprint(f\"Best hyperparameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\nbest_model = grid_search.best_estimator_\ntest_score = best_model.score(x_test, y_test)\nprint(f\"Test set accuracy of the best model: {test_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:46:08.338480Z","iopub.execute_input":"2025-09-09T14:46:08.338931Z","iopub.status.idle":"2025-09-09T14:46:08.458834Z","shell.execute_reply.started":"2025-09-09T14:46:08.338906Z","shell.execute_reply":"2025-09-09T14:46:08.457863Z"}},"outputs":[{"name":"stdout","text":"completed search...\nBest hyperparameters: {'classifier': LogisticRegression(C=0.1, class_weight='balanced', solver='liblinear'), 'classifier__C': 0.1}\nBest cross-validation score: 0.8072\nTest set accuracy of the best model: 0.7241\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"train_set =  x_train_df\ntrain_label = y_train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:48:22.812553Z","iopub.execute_input":"2025-09-09T14:48:22.812910Z","iopub.status.idle":"2025-09-09T14:48:22.817557Z","shell.execute_reply.started":"2025-09-09T14:48:22.812883Z","shell.execute_reply":"2025-09-09T14:48:22.816126Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nimport gc\nNFOLDS = 5\nskf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\noof_classical = np.zeros(len(train_label))\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_set,train_label)):\n    # clear_session()\n    gc.collect()\n    model = LogisticRegression(C=10, class_weight='balanced', solver='liblinear')\n    print(f\" \\n========== FOLD {fold+1}/{NFOLDS} ==========\")\n    x_train_fold = train_set.iloc[train_idx]\n    y_train_fold = train_label[train_idx]\n    x_test_val = train_set.iloc[val_idx]\n    model.fit(x_train_fold,y_train_fold)\n    oof_classical[val_idx] = model.predict_proba(x_test_val)[:,1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:48:25.845631Z","iopub.execute_input":"2025-09-09T14:48:25.846053Z","iopub.status.idle":"2025-09-09T14:48:27.199692Z","shell.execute_reply.started":"2025-09-09T14:48:25.846030Z","shell.execute_reply":"2025-09-09T14:48:27.198864Z"}},"outputs":[{"name":"stdout","text":" \n========== FOLD 1/5 ==========\n \n========== FOLD 2/5 ==========\n \n========== FOLD 3/5 ==========\n \n========== FOLD 4/5 ==========\n \n========== FOLD 5/5 ==========\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"# x_final_test['perplexity_score'] = x_final_test['perplexity_score'].fillna(x_final_test['perplexity_score'].mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T05:18:01.471769Z","iopub.execute_input":"2025-09-08T05:18:01.472792Z","iopub.status.idle":"2025-09-08T05:18:01.478130Z","shell.execute_reply.started":"2025-09-08T05:18:01.472748Z","shell.execute_reply":"2025-09-08T05:18:01.477230Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"model = LogisticRegression(C=1, class_weight='balanced', solver='liblinear')\nmodel.fit(x_train_df,y_train_df)\nx_final_test['perplexity_score'].fillna(x_final_test['perplexity_score'].mean())\ncount_model_test_preds = model.predict_proba(x_final_test)[:,1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:48:38.602649Z","iopub.execute_input":"2025-09-09T14:48:38.602994Z","iopub.status.idle":"2025-09-09T14:48:38.617926Z","shell.execute_reply.started":"2025-09-09T14:48:38.602973Z","shell.execute_reply":"2025-09-09T14:48:38.616453Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# np.save('logreg_count_model_perplexity_llmverd_train.npy',oof_classical)\n# np.save('logreg_count_model_perplexity_llmverd_test.npy',count_model_test_preds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i = 0\ncount_model_test_preds[i]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:54:53.913119Z","iopub.execute_input":"2025-09-09T14:54:53.914080Z","iopub.status.idle":"2025-09-09T14:54:53.920289Z","shell.execute_reply.started":"2025-09-09T14:54:53.914050Z","shell.execute_reply":"2025-09-09T14:54:53.919441Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"0.00044593067874229683"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"count_model_test_preds\ndef label_preds(preds):\n    pred_labels = []\n    j = len(preds)/2\n    for i in range(0,int(len(preds)/2)):\n        if preds[int(i)] > preds[int(j+i)]:\n            pred_labels.append(2)\n        else:\n            pred_labels.append(1)\n    return pred_labels\nfinal_labels = label_preds(count_model_test_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:00:44.239238Z","iopub.execute_input":"2025-09-09T15:00:44.239507Z","iopub.status.idle":"2025-09-09T15:00:44.245470Z","shell.execute_reply.started":"2025-09-09T15:00:44.239486Z","shell.execute_reply":"2025-09-09T15:00:44.244457Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"# final_labels = np.where(np.round(count_model_test_preds)==1,1,2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T05:18:05.457035Z","iopub.execute_input":"2025-09-08T05:18:05.457312Z","iopub.status.idle":"2025-09-08T05:18:05.462511Z","shell.execute_reply.started":"2025-09-08T05:18:05.457293Z","shell.execute_reply":"2025-09-08T05:18:05.461615Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def make_submission_csv(results,name=None):\n    df_results = pd.DataFrame(results)\n    output_df = df_results.copy()\n    output_df.columns = ['real_text_id']\n    output_df.reset_index(inplace=True)\n    output_df.rename(columns={'index': 'id'}, inplace=True)\n    if name!=None:\n        output_df.to_csv(name, index=False)\n    return output_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:56:44.258370Z","iopub.execute_input":"2025-09-09T14:56:44.258619Z","iopub.status.idle":"2025-09-09T14:56:44.264071Z","shell.execute_reply.started":"2025-09-09T14:56:44.258602Z","shell.execute_reply":"2025-09-09T14:56:44.263265Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef compare_submissions(df,prediction_col='real_text_id', id_col='id'):\n    \"\"\"\n    Loads two Kaggle submission CSVs and compares their predictions.\n\n    Args:\n        df (pandas_dataframe): Dataframe of pandas\n        prediction_col (str): The name of the column containing the predictions (1s and 2s).\n        id_col (str): The name of the column containing the sample ID.\n    \"\"\"\n    try:\n        real_df = pd.read_csv('/kaggle/input/fake-or-real-the-imposter-x-train/sample_submission_DEBERTA_ESEMBLED (1).csv')\n        real_df = real_df.sort_values(by=id_col).reset_index(drop=True)\n        \n        df = df.sort_values(by=id_col).reset_index(drop=True)\n\n\n        if len(df) != len(real_df):\n            print(f\"Error: Files have different numbers of rows ({len(df)} vs {len(real_df)}).\")\n            return\n\n\n        preds1 = df[prediction_col]\n        preds2 = real_df[prediction_col]\n\n   \n        num_agreements = (preds1 == preds2).sum()\n        total_predictions = len(df)\n        agreement_rate = num_agreements / total_predictions\n        correct_ans_best = int(1068 * 0.88796)\n        wrong_ans_best = 1068 - correct_ans_best\n        worst_correct_ans_curr = (correct_ans_best * agreement_rate) / 1068\n        best_correct_ans_curr = ((correct_ans_best * agreement_rate) + worst_correct_ans_curr) / 1068\n        print(f\"--- Comparison Report ---\")\n        print(f\"Total Predictions: {total_predictions}\")\n        print(f\"Number of Identical Predictions: {num_agreements}\")\n        print(f\"Number of Changed Predictions: {total_predictions - num_agreements}\")\n        print(f\"Agreement Rate: {agreement_rate:.2%}\")\n        print(f\"Probable Score: {worst_correct_ans_curr:.4%} to {best_correct_ans_curr:.4%}\")\n\n    except FileNotFoundError:\n        print(f\"Error: Could not find one or both of the files. Please check the paths.\")\n    except KeyError:\n        print(f\"Error: One of the files is missing the required column '{prediction_col}' or '{id_col}'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:56:44.489686Z","iopub.execute_input":"2025-09-09T14:56:44.490038Z","iopub.status.idle":"2025-09-09T14:56:44.498064Z","shell.execute_reply.started":"2025-09-09T14:56:44.490010Z","shell.execute_reply":"2025-09-09T14:56:44.497219Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"soln_df = make_submission_csv(final_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:00:54.365384Z","iopub.execute_input":"2025-09-09T15:00:54.365632Z","iopub.status.idle":"2025-09-09T15:00:54.372804Z","shell.execute_reply.started":"2025-09-09T15:00:54.365616Z","shell.execute_reply":"2025-09-09T15:00:54.371618Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"compare_submissions(soln_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:00:55.333988Z","iopub.execute_input":"2025-09-09T15:00:55.334268Z","iopub.status.idle":"2025-09-09T15:00:55.344840Z","shell.execute_reply.started":"2025-09-09T15:00:55.334249Z","shell.execute_reply":"2025-09-09T15:00:55.343816Z"}},"outputs":[{"name":"stdout","text":"--- Comparison Report ---\nTotal Predictions: 1068\nNumber of Identical Predictions: 199\nNumber of Changed Predictions: 869\nAgreement Rate: 18.63%\nProbable Score: 16.5394% to 16.5549%\n","output_type":"stream"}],"execution_count":83},{"cell_type":"markdown","source":"# Meta Model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ntrain_df = pd.read_csv('/kaggle/input/fake-or-real-the-imposter-x-train/train_dataset.csv')\nclassical_preds_train = np.load('/kaggle/input/pred-probas-from-different-models-for-stacking/oof_classical.npy')\nbert_preds_train = np.load('/kaggle/input/pred-probas-from-different-models-for-stacking/oof_bert_512.npy')\nfor_deberta_train = np.load('/kaggle/input/pred-probas-from-different-models-for-stacking/forsenic_deberta_tfidf_feature_model_trainProba.npy')\ncount_model_train = np.load('/kaggle/input/pred-probas-from-different-models-for-stacking/logreg_count_model_train.npy')\nx_meta_train = np.column_stack([classical_preds_train,\n                           bert_preds_train,\n                           for_deberta_train[:93],\n                           count_model_train])\ny_meta_train = np.where(train_df['labels']==1,1,0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classical_preds_test = np.load('/kaggle/input/pred-probas-from-different-models-for-stacking/classical_test_proba.npy')\nbert_preds_test = np.load('/kaggle/input/pred-probas-from-different-models-for-stacking/final_bert_predictions_512_wt_decay.npy')\nfor_deberta_test = np.load('/kaggle/input/pred-probas-from-different-models-for-stacking/forsenic_deberta_tfidf_feature_model_testProba.npy')\ncount_model_preds_test = np.load('/kaggle/input/pred-probas-from-different-models-for-stacking/logreg_count_model_preds_test.npy')\nx_meta_test = np.column_stack([classical_preds_test,\n                               bert_preds_test,\n                               for_deberta_test,\n                               count_model_preds_test])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport xgboost as xgb\nimport lightgbm as lgbm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom scipy.stats import randint, uniform, loguniform\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nX = x_meta_train \ny = y_meta_train\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nmodels_and_params = [\n \n    (xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), {\n        'n_estimators': randint(100, 500),\n        'learning_rate': uniform(0.01, 0.2),\n        'max_depth': randint(3, 10),\n        'subsample': uniform(0.6, 0.4),\n        'colsample_bytree': uniform(0.6, 0.4),\n        'gamma': uniform(0, 0.5)\n    }),\n\n   \n    (lgbm.LGBMClassifier(random_state=42,verbose=-1), {\n        'n_estimators': randint(100, 500),\n        'learning_rate': uniform(0.01, 0.2),\n        'num_leaves': randint(20, 100),\n        'max_depth': randint(3, 15),\n        'subsample': uniform(0.6, 0.4),\n        'colsample_bytree': uniform(0.6, 0.4),\n    }),\n\n\n    (LogisticRegression(solver='liblinear', random_state=42), {\n        'C': loguniform(1e-4, 1e2),\n        'penalty': ['l1', 'l2']\n    })\n]\n\n\nprint(\"Starting Randomized Search for each model...\")\nprint(\"-\" * 50)\n\nfor model, param_dist in models_and_params:\n    try:\n\n        random_search = RandomizedSearchCV(\n            estimator=model,\n            param_distributions=param_dist,\n            n_iter=20,  \n            cv=5,     \n            scoring='accuracy',\n            n_jobs=-1, \n            random_state=42,\n            verbose=0 \n        )\n\n        \n        random_search.fit(X_train, y_train)\n\n        \n        print(f\"Model: {type(model).__name__}\")\n        print(f\"Best cross-validation score: {random_search.best_score_:.4f}\")\n        print(f\"Best parameters found: {random_search.best_params_}\")\n        print(\"-\" * 50)\n\n    except Exception as e:\n        print(f\"An error occurred with model {type(model).__name__}: {e}\")\n        print(\"-\" * 50)\n\nprint(\"Randomized Search completed for all models.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_search.best_estimator_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_stack_model = LogisticRegression(C=0.0012606912518374083, random_state=42, solver='liblinear')\nbest_stack_model.fit(x_meta_train,y_meta_train)\ncount_model_test_preds = best_stack_model.predict_proba(x_meta_test)[:,1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_submission_csv(results):\n    df_results = pd.DataFrame(results)\n    output_df = df_results.copy()\n    output_df.columns = ['real_text_id']\n    output_df.reset_index(inplace=True)\n    output_df.rename(columns={'index': 'id'}, inplace=True)\n    output_df.to_csv('logreg_count_model_perplexity_llmverdict_preds.csv', index=False)\n    return output_df\nmake_submission_csv(final_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}