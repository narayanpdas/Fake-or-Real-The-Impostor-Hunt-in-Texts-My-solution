{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13020947,"sourceType":"datasetVersion","datasetId":8028993}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Preparing the data function**","metadata":{}},{"cell_type":"code","source":"TRAIN_DATA_PATH = '/kaggle/input/fake-or-real-the-imposter-x-train/back_translated_train_dataset.csv' \nimport pandas as pd\nimport numpy as np\nimport random\ndf = pd.read_csv(TRAIN_DATA_PATH)\ndf['text_1'] = df['text_1'].fillna('')\ndf['text_2'] = df['text_2'].fillna('')\ndef shift_randomize_data(df,slide_val):\n    n = len(df)\n    real_texts = pd.concat([\n        df.loc[df['labels'] == 1, 'text_1'],\n        df.loc[df['labels'] == 2, 'text_2']\n    ]).sort_index()\n\n    fake_texts = pd.concat([\n        df.loc[df['labels'] == 1, 'text_2'],\n        df.loc[df['labels'] == 2, 'text_1']\n    ]).sort_index()\n    slided_fake_texts = np.roll(fake_texts.values, -slide_val)\n    part1_df = pd.DataFrame({\n        'text_1': real_texts.values,\n        'text_2': slided_fake_texts,\n        'label': np.zeros(n, dtype=int)\n    })\n    part2_df = pd.DataFrame({\n        'text_1': slided_fake_texts,\n        'text_2': real_texts.values,\n        'label': np.ones(n, dtype=int)\n    })\n    final_df = pd.concat([part1_df, part2_df], ignore_index=True)\n\n    return final_df.sample(frac=1).reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Hyper Parameters**","metadata":{}},{"cell_type":"code","source":"# HYPER PARAMETRS\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification,create_optimizer\nEPOCHS = 4\nLR = 3e-5\nBATCH_SIZE = 4\nWT_DECAY = 0.01\nNUM_MODELS = 5\nSLIDE_PER_RUN = int(len(df) * 0.8 / NUM_MODELS)\nMODEL_NAME = 'microsoft/deberta-v3-base'\nMAX_TOKEN_LEN = 256\nSTATE = 42\nSTRATEGY = tf.distribute.MirroredStrategy()\nMODEL_PATHS = []\nHISTORY_OF_MODELS = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndef tokenize_pack_data(X,y):\n    encodings = tokenizer(\n        X['text_1'].fillna('').tolist(),\n        X['text_2'].fillna('').tolist(),\n        truncation=True,\n        padding=True,\n        max_length=MAX_TOKEN_LEN\n    )\n    labels = y.values\n    dataset = tf.data.Dataset.from_tensor_slices((\n        dict(encodings),\n        labels\n    ))\n    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    return dataset\n    \ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nX = df[['text_1', 'text_2']]\ny = df['labels']\nX_train, X_val, y_train, y_val = train_test_split(\n                                        X, y,\n                                        test_size=0.2,  \n                                        random_state=STATE \n                                      )\ny_val = y_val - 1\nval_dataset = tokenize_pack_data(X_val,y_val)\ntrain_df = pd.concat([X_train,y_train],axis=1).sort_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Model Training**","metadata":{}},{"cell_type":"code","source":"import gc\nfrom transformers import Trainer, TrainingArguments\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfor i in range(NUM_MODELS):\n    shift_val = i * SLIDE_PER_RUN\n    print(f\"--- Training Model {i+1}/{NUM_MODELS} with a shift of {shift_val} ---\")\n    shifted_train_df = shift_randomize_data(train_df,shift_val)\n    train_dataset = tokenize_pack_data(shifted_train_df[['text_1', 'text_2']],\n                                       shifted_train_df['label']\n                                      )\n    with STRATEGY.scope():\n        model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME,num_labels=1)\n        num_train_steps = len(train_dataset) * EPOCHS\n        optimizer, _ = create_optimizer(init_lr=LR, \n                                        num_warmup_steps=int(0.1 * num_train_steps), \n                                        num_train_steps=num_train_steps\n                                        )\n        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n        model.compile(optimizer=optimizer,loss=loss, metrics=['accuracy'])\n    model_path = f\"./my_trained_model_{i}\"\n    MODEL_PATHS.append(model_path)\n    history = model.fit(\n        train_dataset,\n        epochs=EPOCHS,\n        validation_data=val_dataset,\n    )\n    model.save_pretrained(model_path)\n    tokenizer.save_pretrained(model_path)\n    HISTORY_OF_MODELS.append(history)\n    # TRAINED_MODELS.append(model)\n    tf.keras.backend.clear_session()\n    gc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_encodings=tokenizer(\n        X_val['text_1'].fillna('').tolist(),\n        X_val['text_2'].fillna('').tolist(),\n        truncation=True,\n        padding=True,\n        max_length=MAX_TOKEN_LEN,\n        return_tensors=\"tf\"\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ntest_df = pd.read_csv('/kaggle/input/fake-or-real-the-imposter-x-train/test_data.csv')\ntest_encodings=tokenizer(\n        test_df['text_1'].fillna('').tolist(),\n        test_df['text_2'].fillna('').tolist(),\n        truncation=True,\n        padding=True,\n        max_length=MAX_TOKEN_LEN,\n        return_tensors=\"tf\"\n    )\nall_logits = []\nfor path in MODEL_PATHS:\n    model = TFAutoModelForSequenceClassification.from_pretrained(path, num_labels=1)\n    print(f\"Loading weights from {path}...\")    \n    logits = model.predict(dict(test_encodings)).logits\n    all_logits.append(tf.nn.sigmoid(logits).numpy().flatten())\npreds_proba = np.mean(all_logits,axis=0)\npreds_labels = np.round(preds_proba)\nensemble_accuracy = accuracy_score(y_val, preds_proba) \nprint(f\"\\nFinal Ensemble Validation Accuracy: {ensemble_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = np.where(preds_labels==0,1,2)\ndef make_submission_csv(results):\n    df_results = pd.DataFrame(results)\n    output_df = df_results.copy()\n    output_df.columns = ['real_text_id']\n    output_df.reset_index(inplace=True)\n    output_df.rename(columns={'index': 'id'}, inplace=True)\n    output_df.to_csv('5_BERTs_on_shiffting_data_debiasing_BT.csv', index=False)\n    return output_df\nmake_submission_csv(labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Smart Combining**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef create_hard_negative_dataset(train_pool_df: pd.DataFrame, top_k: int = 3) -> pd.DataFrame:\n    \"\"\"\n    Performs hard negative mining to create a high-quality, debiased training set.\n\n    For each real text in the training pool, it finds the 'top_k' most similar\n    fake texts and creates debiased pairs for training.\n\n    Args:\n        train_pool_df: The DataFrame containing the training portion of the data.\n                       Must have 'text_1', 'text_2', and 'labels' columns.\n        top_k: The number of most similar fakes to pair with each real text.\n\n    Returns:\n        A new DataFrame with debiased (real, fake) and (fake, real) pairs,\n        ready for model training.\n    \"\"\"\n    print(\"Starting hard negative mining data augmentation...\")\n    real_texts = pd.concat([\n        train_pool_df.loc[train_pool_df['labels'] == 1, 'text_1'],\n        train_pool_df.loc[train_pool_df['labels'] == 2, 'text_2']\n    ]).sort_index().tolist()\n\n    fake_texts = pd.concat([\n        train_pool_df.loc[train_pool_df['labels'] == 1, 'text_2'],\n        train_pool_df.loc[train_pool_df['labels'] == 2, 'text_1']\n    ]).sort_index().tolist()\n    print(\"Generating text embeddings...\")\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    real_embeddings = model.encode(real_texts, show_progress_bar=True)\n    fake_embeddings = model.encode(fake_texts, show_progress_bar=True)\n\n    print(\"Calculating similarity and finding hard negatives...\")\n\n    similarity_matrix = cosine_similarity(real_embeddings, fake_embeddings)\n\n    hard_negative_indices = np.argsort(similarity_matrix, axis=1)[:, -top_k:]\n\n    augmented_pairs = []\n    for i in range(len(real_texts)):\n        real_text = real_texts[i]\n        for fake_idx in hard_negative_indices[i]:\n            hard_fake_text = fake_texts[fake_idx]\n            augmented_pairs.append({'text_1': real_text, 'text_2': hard_fake_text, 'label': 1})\n            augmented_pairs.append({'text_1': hard_fake_text, 'text_2': real_text, 'label': 0})\n            \n    final_df = pd.DataFrame(augmented_pairs)\n    print(f\"Data augmentation complete. Generated {len(final_df)} training samples.\")\n    return final_df.sample(frac=1).reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nPATH = '/kaggle/input/fake-or-real-the-imposter-x-train/train_dataset.csv'\nSTATE =42\ndf = pd.read_csv(PATH)\ntrain_df ,val_df = train_test_split(df,test_size=0.15,random_state=STATE)\n\nval_df['labels'] = np.where(val_df['labels']==1,1,0)\n\naugmented_train_df = create_hard_negative_dataset(train_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification,create_optimizer\nEPOCHS = 4\nLR = 2.5e-5\nBATCH_SIZE = 4\nWT_DECAY = 0.01\nNUM_MODELS = 5\nSLIDE_PER_RUN = int(len(df) * 0.8 / NUM_MODELS)\nMODEL_NAME = 'microsoft/deberta-v3-base'\nMAX_TOKEN_LEN = 256\nSTRATEGY = tf.distribute.MirroredStrategy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_pack_data(X,y):\n    encodings = tokenizer(\n        X['text_1'].fillna('').tolist(),\n        X['text_2'].fillna('').tolist(),\n        truncation=True,\n        padding=True,\n        max_length=MAX_TOKEN_LEN\n    )\n    labels = y.values\n    dataset = tf.data.Dataset.from_tensor_slices((\n        dict(encodings),\n        labels\n    ))\n    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with STRATEGY.scope():\n        model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME,num_labels=1)\n        num_train_steps = len(augmented_train_df) * EPOCHS\n        optimizer, _ = create_optimizer(init_lr=LR, \n                                        num_warmup_steps=int(0.1 * num_train_steps), \n                                        num_train_steps=num_train_steps,\n                                        weight_decay_rate=WT_DECAY\n                                        )\n        # loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n        model.compile(optimizer=optimizer, metrics=['accuracy'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\naugmented_train_dataset = tokenize_pack_data(augmented_train_df[['text_1','text_2']],\n                                             augmented_train_df['label'])\nval_dataset = tokenize_pack_data(val_df[['text_1','text_2']],\n                                 val_df['labels'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n        augmented_train_dataset,\n        epochs=EPOCHS,\n        validation_data=val_dataset,\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/fake-or-real-the-imposter-x-train/test_data.csv')\ntest_encodings = tokenizer(\n        test_df['text_1'].fillna('').tolist(),\n        test_df['text_2'].fillna('').tolist(),\n        truncation=True,\n        padding=True,\n        max_length=MAX_TOKEN_LEN,\n        return_tensors=\"tf\"\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_logits = model.predict(dict(test_encodings)).logits\ntest_proba = tf.nn.sigmoid(test_logits).numpy().flatten()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_proba","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_labels = np.where(test_proba>0.5,1,2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_submission_csv(results):\n    df_results = pd.DataFrame(results)\n    output_df = df_results.copy()\n    output_df.columns = ['real_text_id']\n    output_df.reset_index(inplace=True)\n    output_df.rename(columns={'index': 'id'}, inplace=True)\n    output_df.to_csv('DEBERTa_on_hard_negative_dataset(k=3).csv', index=False)\n    return output_df\nmake_submission_csv(test_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}