{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13065351,"sourceType":"datasetVersion","datasetId":8268983},{"sourceId":13072159,"sourceType":"datasetVersion","datasetId":8100028},{"sourceId":13080736,"sourceType":"datasetVersion","datasetId":8028993}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Hyper Parameters\nimport tensorflow as tf\nfrom transformers import AutoTokenizer,TFAutoModelForSequenceClassification,create_optimizer,AutoConfig\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\n\nLR = 1e-6\nMAX_TOKEN_LEN = 256\nNUM_EPOCHS = 2\nPATIENCE = 2  \nBATCH_SIZE = 1\nORIGINAL_WEIGHT = 1.0\nPSEUDO_WEIGHT = 0.75 \nWT_DECAY = 0.1\nCONFIDENCE_THRESH = 0.85 \nAGREEMENT_THRESH = 0.10\nCONFIDENCE_WEIGHT = 0.7\nNUM_PSEUDO_PAIRS = 200\nSTRATEGY = tf.distribute.MirroredStrategy()\nSAVE_PATH = f'./best_models'\nMODEL_NAME = 'roberta-large'\nPRED_SAVE_PATH = './pred_proba'\nSAVED_PATHS = []\nMODEL_NAMES = [\n    'albert-base-v2',\n    'distilbert-base-uncased',\n    'google/mobilebert-uncased'\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reinforcement Data Generation","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nsbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n\ndef get_semantic_similarity(text1, text2):\n    \"\"\"\n    Calculates the semantic similarity between two texts using sentence embeddings.\n    \"\"\"\n    embeddings = sbert_model.encode([text1, text2])\n    return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\ntotal_similarity = []\nfor _,row in test_df.iterrows():\n    similarity = get_semantic_similarity(row['text_2'],row['text_1'])\n    total_similarity.append(similarity)\ntest_df['similarity_score'] = total_similarity","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = test_df.sort_values(by='similarity_score',ascending=False)\n\nids_similar = test_df[:35]['id']\nids_psuedo  = pseudo_train_data['id']\nids_to_exclude = pd.concat([ids_similar, ids_psuedo]).unique()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"deberta_test_proba = pd.read_csv('/kaggle/input/pred-probas-from-different-models-for-stacking/deberta_large_READX200_test_proba.csv')\n\ndeberta_test_proba['confidence'] = (deberta_test_proba['deberta_large_READX200_prob1'] - deberta_test_proba['deberta_large_READX200_prob2']).abs()\ndeberta_test_proba = debrta_test_proba.sort_values(by='confidence')\n\ndeberta_test_proba['id'] = deberta_test_proba['Unnamed: 0']\ndeberta_test_proba.drop(['Unnamed: 0'],axis=1,inplace=True)\n\ndeberta_test_proba_filtered = deberta_test_proba[~deberta_test_proba['id'].isin(ids_to_exclude)]","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"org_test_df = pd.read_csv(TEST_DATA_PATH)\nreinforcement_test_data = org_test_df[org_test_df['id'].isin(deberta_test_proba_filtered.head(200)['id'])]","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reinforcement_test_data.to_csv('reinforcement_test_data_200_version.csv')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the Data and Using the Manual Train Loop","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_DATA_PATH)\ntest_df = pd.read_csv(TEST_DATA_PATH)\npsuedo_train_df = pd.read_csv(PSUEDO_DATA_PATH)\nreinforcement_train_data = pd.read_csv(RT_PATH)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pack_tokenize_dataset(combined_df, \n                          reinforced_df,\n                          tokenizer, \n                          batch_size, \n                          max_len):\n    \"\"\"Combines all data sources and prepares train and val tf.data.Dataset for the custom loop.\"\"\"\n    orginal_train_df = combined_df[combined_df['category']=='original']\n    psuedo_df = combined_df[combined_df['category']=='psuedo']\n    \n    original_df,val_df = train_test_split(orginal_train_df,\n                                        test_size=0.15,\n                                        random_state=42,\n                                        startegy=orginal_train_df['label'])\n    \n    original_df['sample_weight'] = ORIGINAL_WEIGHT\n    pseudo_df['sample_weight'] = PSEUDO_WEIGHT\n    labeled_df = pd.concat([original_df, pseudo_df], ignore_index=True)\n    \n    unlabeled_df = reinforced_df.copy()\n    unlabeled_df['label'] = -1\n    unlabeled_df['sample_weight'] = 0.0\n    \n    combined_df = pd.concat([labeled_df, unlabeled_df], ignore_index=True).sample(frac=1).reset_index(drop=True)\n\n    train_encodings = tokenizer(combined_df['text'].tolist(), \n                                truncation=True, \n                                padding=True, \n                                max_length=max_len)\n    \n    train_dataset = tf.data.Dataset.from_tensor_slices((\n        dict(train_encodings),\n        combined_df['label'].values,\n        combined_df['sample_weight'].values.astype('float32')\n    ))\n    val_encodings = tokenizer(val_df['text'].tolist(), \n                          truncation=True, \n                          padding=True, \n                          max_length=max_len)\n    val_dataset = tf.data.Dataset.from_tensor_slices((\n        dict(val_encodings)\n    ))\n    return train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE),val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model_parts(model_name, \n                    lr, \n                    weight_decay, \n                    num_train_steps, \n                    num_warmup_steps):\n    \"\"\"Initializes the model and a suitable optimizer.\"\"\"\n    \n    config = AutoConfig.from_pretrained(model_name)\n    config.num_labels = 2\n    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n    optimizer, _ = create_optimizer(\n        init_lr=lr,\n        num_train_steps=num_train_steps,\n        num_warmup_steps=num_warmup_steps,\n        weight_decay_rate=weight_decay\n    )\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    return model, optimizer ,tokenizer","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function\ndef train_step(model, \n               batch, \n               optimizer, \n               supervised_loss_fn, \n               confidence_weight):\n    \"\"\"Performs one training step with a supervised and unsupervised loss.\"\"\"\n    inputs, labels, sample_weights = batch\n\n    with tf.GradientTape() as tape:\n        logits = model(inputs, training=True).logits\n        \n        labeled_mask = tf.not_equal(labels, -1)\n        unlabeled_mask = tf.equal(labels, -1)\n        \n\n        labeled_logits = tf.boolean_mask(logits, labeled_mask)\n        labeled_labels = tf.boolean_mask(labels, labeled_mask)\n        labeled_weights = tf.boolean_mask(sample_weights, labeled_mask)\n\n        per_sample_loss = supervised_loss_fn_no_reduction(labeled_labels, labeled_logits)\n        supervised_loss = tf.reduce_mean(per_sample_loss * labeled_weights)\n        \n        unlabeled_logits = tf.boolean_mask(logits, unlabeled_mask)\n        if tf.shape(unlabeled_logits)[0] > 0:\n            probs_unlabeled = tf.nn.softmax(unlabeled_logits)\n            confidence_loss = -tf.reduce_mean(tf.reduce_sum(probs_unlabeled * tf.math.log(probs_unlabeled + 1e-9), axis=-1))\n        else:\n            confidence_loss = 0.0\n\n        total_loss = supervised_loss + confidence_weight * tf.cast(confidence_loss, dtype=supervised_loss.dtype)\n\n    gradients = tape.gradient(total_loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n    return total_loss ,logits\n@tf.function\ndef validation_step(model, batch, loss_fn):\n    \"\"\"Performs one step of validation.\"\"\"\n    \n    inputs, labels = batch\n    logits = model(inputs, training=False).logits\n    loss = loss_fn(labels, logits)\n    return loss, logits","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_training(\n    model_name,\n    combined_df,\n    reinforced_df,\n    epochs,\n    save_path\n):\n    \"\"\"Orchestrates the full training and validation loop for a model.\"\"\"\n    best_val_loss = float('inf')\n    with STRATEGY.scope():        \n        model, optimizer,tokenizer = get_model_parts(\n            model_name=model_name,\n            lr=LR,\n            weight_decay=WT_DECAY,\n            num_train_steps=num_train_steps,\n            num_warmup_steps=num_warmup_steps\n        )\n        supervised_loss_fn_no_reduction = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n        val_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    train_dataset,val_dataset = pack_tokenize_dataset(combined_df,\n                                                      reinforced_df,\n                                                      tokenizer, \n                                                      batch_size, \n                                                      max_len)\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        model.trainable = True\n        for batch in tqdm(train_dataset, desc=\"Training\"):\n            loss, _ = train_step(model,\n                                 batch, \n                                 optimizer, \n                                 supervised_loss_fn_no_reduction,\n                                 confidence_weight)\n        val_losses = []\n        all_val_preds = []\n        all_val_labels = []\n        for batch in tqdm(val_dataset, desc=\"Validating\"):\n            val_loss, val_logits = validation_step(model, \n                                                   batch, \n                                                   val_loss_fn)\n            val_losses.append(val_loss)\n            all_val_preds.extend(np.argmax(val_logits, axis=1))\n            all_val_labels.extend(batch[1].numpy())\n            \n        avg_val_loss = np.mean(val_losses)\n        val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n        \n        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n        if avg_val_loss < best_val_loss:\n            print(f\"Validation loss improved. Saving model to {save_path}\")\n            best_val_loss = avg_val_loss\n            model.save_pretrained(save_path)\n    return model","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = run_training(\n    model_name = MODEL_NAME,\n    optimizer = optimizer,\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    epochs=NUM_EPOCHS,\n    save_path= SAVE_PATH\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification, create_optimizer, AutoConfig\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score\nimport gc\n\nTEST_DATA_PATH = ''\nTRAIN_DATA_PATH = ''\nPSEUDO_TRAIN_DATA_PATH = ''\nRT_PATH = ''\n\n# --- 1. Hyperparameters and Configuration ---\nDEBUG = False\nLR = 1.5e-5\nMAX_LEN = 256  # Using 256 for better memory performance with large models\nNUM_EPOCHS = 3\nPATIENCE = 2\nBATCH_SIZE_PER_REPLICA = 2 # This is the batch size for each GPU\nORIGINAL_WEIGHT = 1.0\nPSEUDO_WEIGHT = 0.75\nCONFIDENCE_WEIGHT = 0.5 # Weight for the unsupervised loss\nWT_DECAY = 0.1\nMODEL_PATH = '/kaggle/input/deberta-large-models/deberta_large_on_READX200'\nMODEL_NAME = MODEL_PATH \nSAVE_PATH = f'./final_model'\nSTATE = 42\nstrategy = tf.distribute.MirroredStrategy()\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T13:49:55.705333Z","iopub.execute_input":"2025-09-17T13:49:55.705583Z","iopub.status.idle":"2025-09-17T13:50:02.918602Z","shell.execute_reply.started":"2025-09-17T13:49:55.705562Z","shell.execute_reply":"2025-09-17T13:50:02.917653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef augment_data(df):\n    \"\"\"Unpairs a DataFrame of paired texts into a single-text classification format.\"\"\"\n    if 'labels' in df.columns or 'label' in df.columns:\n        # --- Labeled Data Logic (Optimized) ---\n        label_col = 'labels' if 'labels' in df.columns else 'label'\n        all_texts = pd.concat([df['text_1'], df['text_2']], ignore_index=True)\n        labels_for_text1 = np.where(df[label_col] == 1, 1, 0)\n        labels_for_text2 = np.where(df[label_col] == 1, 0, 1)  \n        all_labels = np.concatenate([labels_for_text1, labels_for_text2])\n        return pd.DataFrame({'text': all_texts, 'label': all_labels})\n\n    else:\n        all_texts = pd.concat([df['text_1'], df['text_2']], ignore_index=True)\n        all_ids = pd.concat([df['id'], df['id']], ignore_index=True)\n        original_pos = ['text_1'] * len(df) + ['text_2'] * len(df)\n\n        return pd.DataFrame({\n            'id': all_ids,\n            'original_pos': original_pos,\n            'text': all_texts\n        })\n        \n\ndef create_datasets(original_train_df, pseudo_df, reinforcement_df, val_df, tokenizer,batch_size):\n    \"\"\"Creates the final tf.data.Dataset objects for training and validation.\"\"\"\n\n    original_train_df['sample_weight'] = ORIGINAL_WEIGHT\n    pseudo_df['sample_weight'] = PSEUDO_WEIGHT\n    labeled_df = pd.concat([original_train_df, pseudo_df], ignore_index=True)\n    \n\n    unlabeled_df = reinforcement_df.copy()\n    unlabeled_df['label'] = -1\n    unlabeled_df['sample_weight'] = 0.0\n    \n\n    final_train_df = pd.concat([labeled_df, \n                                unlabeled_df], \n                               ignore_index=True).sample(frac=1, \n                                                         random_state=STATE).reset_index(drop=True)\n\n\n    train_encodings = tokenizer(final_train_df['text'].fillna('').tolist(), \n                                truncation=True, \n                                padding=True,\n                                max_length=MAX_LEN)\n    train_dataset = tf.data.Dataset.from_tensor_slices((\n        dict(train_encodings),\n        final_train_df['label'].values,\n        final_train_df['sample_weight'].values.astype('float32')\n    )).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    val_encodings = tokenizer(val_df['text'].fillna('').tolist(), \n                              truncation=True, \n                              padding=True, \n                              max_length=MAX_LEN)\n    val_dataset = tf.data.Dataset.from_tensor_slices((\n        dict(val_encodings),\n        val_df['label'].values\n    )).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    \n    return train_dataset, val_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_original = pd.read_csv(TRAIN_DATA_PATH)\ndf_pseudo = pd.read_csv(PSEUDO_TRAIN_DATA_PATH)\ndf_reinforcement  = pd.read_csv(RT_PATH)\n\noriginal_train_df, original_val_df = train_test_split(df_original, test_size=0.25, random_state=STATE, stratify=df_original['labels'])\naug_original_train = augment_data(original_train_df)\naug_val = augment_data(original_val_df)\naug_pseudo = augment_data(df_pseudo)\naug_reinforce = augment_data(df_reinforcement) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if DEBUG:\n    print(\"=\"*30)\n    print(\"--- RUNNING IN DEBUG MODE ---\")\n    print(\"=\"*30)\n    aug_original_train = aug_original_train.sample(n=8, random_state=42).reset_index(drop=True)\n    aug_val = aug_val.sample(n=4, random_state=42).reset_index(drop=True)\n    aug_pseudo = aug_pseudo.sample(n=10, random_state=42).reset_index(drop=True)\n    aug_reinforce = aug_reinforce.sample(n=10, random_state=42).reset_index(drop=True)\n    NUM_EPOCHS = 1\n    MAX_TOKEN_LEN = 128\nelse:\n    print(\"=\"*30)\n    print(\"--- RUNNING IN NORMAL MODE ---\")\n    print(\"=\"*30)\n\n\nwith strategy.scope():\n        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False if 'deberta-v3' in MODEL_NAME else True)\n        num_train_steps = (len(aug_original_train) + len(aug_pseudo) + len(aug_reinforce)) // GLOBAL_BATCH_SIZE * NUM_EPOCHS\n        \n        config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=2)\n        model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n        optimizer, _ = create_optimizer(init_lr=LR, num_warmup_steps=0, num_train_steps=num_train_steps, weight_decay_rate=WT_DECAY)\n        \n        supervised_loss_fn_no_reduction = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n        val_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n                                                                    reduction=tf.keras.losses.Reduction.NONE)\n\n@tf.function\ndef train_step(model, optimizer, supervised_loss_fn, confidence_weight, batch):\n        features, labels, sample_weights = batch\n        with tf.GradientTape() as tape:\n            logits = model(features, training=True).logits\n            \n            labeled_mask = tf.not_equal(labels, -1)\n            labeled_logits = tf.boolean_mask(logits, labeled_mask)\n            labeled_labels = tf.boolean_mask(labels, labeled_mask)\n            labeled_weights = tf.boolean_mask(sample_weights, labeled_mask)\n            \n            per_sample_loss = supervised_loss_fn(labeled_labels, labeled_logits)\n            supervised_loss = tf.nn.compute_average_loss(per_sample_loss, sample_weight=labeled_weights)\n            \n            unlabeled_logits = tf.boolean_mask(logits, tf.equal(labels, -1))\n            if tf.shape(unlabeled_logits)[0] > 0:\n                probs_unlabeled = tf.nn.softmax(unlabeled_logits)\n                confidence_loss = -tf.reduce_mean(tf.reduce_sum(probs_unlabeled * tf.math.log(probs_unlabeled + 1e-9), axis=-1))\n            else:\n                confidence_loss = 0.0\n    \n            total_loss = supervised_loss + confidence_weight * tf.cast(confidence_loss, dtype=supervised_loss.dtype)\n            \n        gradients = tape.gradient(total_loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        return total_loss, logits\n\n@tf.function\ndef validation_step(inputs):\n        \"\"\"Performs one distributed validation step.\"\"\"\n        features, labels = inputs\n        logits = model(features, training=False).logits\n        per_sample_loss = val_loss_fn(labels, logits)\n        loss = tf.nn.compute_average_loss(per_sample_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n        return loss, logits, labels\n\ntrain_dataset, val_dataset = create_datasets(aug_original_train, \n                                             aug_pseudo, \n                                             aug_reinforce, \n                                             aug_val, \n                                             tokenizer, \n                                             batch_size=GLOBAL_BATCH_SIZE)\n\nbest_val_loss = float('inf')\npatience_counter = 0\n\nfor epoch in range(NUM_EPOCHS):    \n    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n    running_loss = 0.0\n    pbar = tqdm(train_dataset, desc=\"Training\")\n    for i,batch in enumerate(pbar):\n        per_replica_loss, per_replica_logits = strategy.run(\n            train_step, \n            args=(model, optimizer, supervised_loss_fn_no_reduction, CONFIDENCE_WEIGHT, batch)\n        )\n        batch_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)    \n        mean_batch_loss = batch_loss / strategy.num_replicas_in_sync\n        running_loss = (running_loss * i + mean_batch_loss.numpy()) / (i + 1)\n            \n        pbar.set_postfix({'training_loss': f'{running_loss:.4f}'})\n        \n    \n    val_losses, all_val_preds, all_val_labels = [], [], []\n    dist_val_dataset = strategy.experimental_distribute_dataset(val_dataset)\n    for per_replica_batch  in tqdm(dist_val_dataset, desc=\"Validating\"):\n        per_replica_loss, per_replica_logits, per_replica_labels = strategy.run(\n            validation_step, args=(per_replica_batch,)\n        )\n        \n        gathered_labels = strategy.gather(per_replica_labels, axis=0)\n        gathered_logits = strategy.gather(per_replica_logits, axis=0)\n        \n\n        loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n        \n\n        val_losses.append(loss.numpy())\n        all_val_preds.extend(np.argmax(gathered_logits, axis=1))\n        all_val_labels.extend(gathered_labels.numpy())\n    avg_val_loss = np.mean(val_losses) / strategy.num_replicas_in_sync\n    val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n    \n    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n    if avg_val_loss < best_val_loss:\n        print(f\"Validation loss improved. Saving model to {SAVE_PATH}\")\n        \n        best_val_loss = avg_val_loss\n        model.save_pretrained(SAVE_PATH)\n        tokenizer.save_pretrained(SAVE_PATH)\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        print(f\"Patience: {patience_counter}/{PATIENCE}\")\n    \n    if patience_counter >= PATIENCE:\n        print(\"Early stopping triggered.\")\n        break\n    gc.collect()\n\nprint(\"\\n--- Pipeline Complete ---\")","metadata":{"trusted":true,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pack_data(test_df_text, tokenizer, mode='pred'):\n    encodings = tokenizer(test_df_text['text'].fillna('').tolist(), \n                                truncation=True, \n                                padding=True,\n                                max_length=MAX_LEN)\n    dataset = tf.data.Dataset.from_tensor_slices((\n        dict(encodings),\n    )).batch(1).prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T13:50:02.919811Z","iopub.execute_input":"2025-09-17T13:50:02.920273Z","iopub.status.idle":"2025-09-17T13:50:02.924960Z","shell.execute_reply.started":"2025-09-17T13:50:02.920252Z","shell.execute_reply":"2025-09-17T13:50:02.924240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/fake-or-real-the-imposter-x-train/test_data.csv')\nCURR_SAVE_PATHS = ['/kaggle/working/final_model']\ntest_df_text1 = test_df[['text_1']].rename(columns={'text_1': 'text'})\ntest_df_text2 = test_df[['text_2']].rename(columns={'text_2': 'text'})\nmodel_predictions = {}\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nfor model_path in CURR_SAVE_PATHS:\n    print(f'---------------predicting with saved model{model_path}---------------')\n    print('---------------Loading Model and Tokenizer---------------')\n    with strategy.scope():\n        model = TFAutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    print('---------------Tokenizing and Predicting on Text1---------------')\n    \n    test_dataset1 = pack_data(test_df_text1, tokenizer, mode='pred')\n    logits1 = model.predict(test_dataset1).logits\n    probs1 = tf.nn.softmax(logits1)[:, 1].numpy() \n    \n    print('---------------Tokenizing and Predicting on Text2---------------')\n    test_dataset2 = pack_data(test_df_text2, tokenizer, mode='pred')\n    logits2 = model.predict(test_dataset2).logits\n    probs2 = tf.nn.softmax(logits2)[:, 1].numpy() \n\n    model_predictions[f'{model_path}_prob1'] = probs1\n    model_predictions[f'{model_path}_prob2'] = probs2\n    print('---------------Finished Model Predictions---------------')\n    tf.keras.backend.clear_session()\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T13:50:02.925659Z","iopub.execute_input":"2025-09-17T13:50:02.925851Z","iopub.status.idle":"2025-09-17T13:53:54.212527Z","shell.execute_reply.started":"2025-09-17T13:50:02.925835Z","shell.execute_reply":"2025-09-17T13:53:54.211682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_df = pd.DataFrame(model_predictions)\npredictions_df['id'] = test_df['id'] \nPRED_SAVE_PATH = 'deberta(0.91493)_large_on_RL_system_.csv'\npredictions_df.to_csv(PRED_SAVE_PATH, index=False)\nprint(f\"\\nIndividual model predictions saved to {PRED_SAVE_PATH}\")\nprint(predictions_df.head())\n\n\nprob1_cols = [col for col in predictions_df.columns if '_prob1' in col]\nprob2_cols = [col for col in predictions_df.columns if '_prob2' in col]\n\nfinal_avg_probs_text1 = predictions_df[prob1_cols].mean(axis=1)\nfinal_avg_probs_text2 = predictions_df[prob2_cols].mean(axis=1)\n\nfinal_labels = np.where(final_avg_probs_text1 > final_avg_probs_text2, 1, 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T13:54:10.609051Z","iopub.execute_input":"2025-09-17T13:54:10.609362Z","iopub.status.idle":"2025-09-17T13:54:10.641506Z","shell.execute_reply.started":"2025-09-17T13:54:10.609337Z","shell.execute_reply":"2025-09-17T13:54:10.640772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_submission_csv(results,name=None):\n    df_results = pd.DataFrame(results)\n    output_df = df_results.copy()\n    output_df.columns = ['real_text_id']\n    output_df.reset_index(inplace=True)\n    output_df.rename(columns={'index': 'id'}, inplace=True)\n    if name!=None:\n        output_df.to_csv(name, index=False)\n    return output_df\npred_df = make_submission_csv(final_labels,name='Deberta_large_on_RL_system_.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T13:54:11.499428Z","iopub.execute_input":"2025-09-17T13:54:11.500135Z","iopub.status.idle":"2025-09-17T13:54:11.507375Z","shell.execute_reply.started":"2025-09-17T13:54:11.500102Z","shell.execute_reply":"2025-09-17T13:54:11.506662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !zip -r best_deberta_large_models.zip /kaggle/working/best_models_fine_tuned_fine_tuned\npred_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T13:54:16.765283Z","iopub.execute_input":"2025-09-17T13:54:16.766047Z","iopub.status.idle":"2025-09-17T13:54:16.782291Z","shell.execute_reply.started":"2025-09-17T13:54:16.766021Z","shell.execute_reply":"2025-09-17T13:54:16.781535Z"}},"outputs":[],"execution_count":null}]}