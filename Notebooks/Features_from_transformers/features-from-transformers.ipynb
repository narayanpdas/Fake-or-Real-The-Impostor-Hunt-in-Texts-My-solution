{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":105874,"databundleVersionId":12964783,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":12978443,"sourceType":"datasetVersion","datasetId":8028993}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **LLM Verdict Feature**","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers accelerate bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:55:40.324197Z","iopub.execute_input":"2025-09-08T12:55:40.324445Z","iopub.status.idle":"2025-09-08T12:56:59.152804Z","shell.execute_reply.started":"2025-09-08T12:55:40.324426Z","shell.execute_reply":"2025-09-08T12:56:59.152054Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom tqdm.auto import tqdm\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"LLAMA_3_TOKEN\")\nlogin(token=secret_value_0)\nprint(\"Successfully logged in to Hugging Face!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:57:01.609851Z","iopub.execute_input":"2025-09-08T12:57:01.610325Z","iopub.status.idle":"2025-09-08T12:57:10.252345Z","shell.execute_reply.started":"2025-09-08T12:57:01.610297Z","shell.execute_reply":"2025-09-08T12:57:10.251608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ntrain_data_path = '/kaggle/input/fake-or-real-the-imposter-x-train/train_dataset.csv'\ntest_data_path = '/kaggle/input/fake-or-real-the-imposter-x-train/test_data.csv'\ndf = pd.read_csv(test_data_path)\ndf['text_1'] = df['text_1'].fillna('')\ndf['text_2'] = df['text_2'].fillna('')\nnew_df = pd.DataFrame()\nnew_df['text'] = pd.concat([df['text_1'],df['text_2']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:57:15.578448Z","iopub.execute_input":"2025-09-08T12:57:15.579161Z","iopub.status.idle":"2025-09-08T12:57:16.121344Z","shell.execute_reply.started":"2025-09-08T12:57:15.579135Z","shell.execute_reply":"2025-09-08T12:57:16.120792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nprint(\"Loading quantized model and tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=quantization_config,\n    device_map=\"auto\", \n)\nprint(\"Model loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:57:38.083154Z","iopub.execute_input":"2025-09-08T12:57:38.083428Z","iopub.status.idle":"2025-09-08T13:00:59.255007Z","shell.execute_reply.started":"2025-09-08T12:57:38.083406Z","shell.execute_reply":"2025-09-08T13:00:59.254363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def llm_verdict(new_df):\n    SYSTEM_PROMPT = \"\"\"You are a highly discerning forensic editor. Your task is to analyze the following text and determine how likely it is to be 'REAL' (closer to a well-written original source) versus 'FAKE' (a more distorted, lower-quality modification).\n    Consider the text's specificity, factual consistency, tone, and complexity.\n    \"\"\"\n    batch_size = 1  \n    verdicts = []\n    torch.cuda.empty_cache()\n    gc.collect()\n                \n    \n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    print(f\"Starting inference on {len(new_df)} samples with batch size {batch_size}...\")\n    for i in tqdm(range(0, len(new_df), batch_size)):\n        batch_df = new_df.iloc[i:i+batch_size]\n        prompts = [\n            tokenizer.apply_chat_template([\n                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"\"\"\n                        ---\n                        Text:\n                        ---\n                        {row['text']}\n                        ---\n                        Based on your expert analysis, provide a confidence score from 0 to 100 on how likely the text is to be REAL, where 100 is completely confident it is REAL and 0 is completely confident it is FAKE. State ONLY the number.\n                        \"\"\"\n                    }\n                ],\n                tokenize=False,\n                add_generation_prompt=True\n            )\n            for _, row in batch_df.iterrows()\n        ]\n        \n\n        inputs = tokenizer(prompts,\n                           return_tensors=\"pt\", \n                           padding=True, \n                           truncation=False).to(\"cuda\")\n        \n\n        output_sequences = model.generate(\n            **inputs,\n            temperature=0.01,\n            max_new_tokens=12,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        \n\n        responses = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n        for response in responses:\n            llm_verdict = response.split(\"assistant\\n\\n\")[-1]\n            verdicts.append(llm_verdict)\n        torch.cuda.empty_cache()\n        gc.collect()\n    return verdicts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:43:41.805429Z","iopub.execute_input":"2025-09-08T13:43:41.806026Z","iopub.status.idle":"2025-09-08T13:43:41.813059Z","shell.execute_reply.started":"2025-09-08T13:43:41.806001Z","shell.execute_reply":"2025-09-08T13:43:41.812331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# new_df['llm_judge_verdict'] = llm_verdict(new_df)\n# print('LLm Judging Completed...')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:43:46.375022Z","iopub.execute_input":"2025-09-08T13:43:46.375595Z","iopub.status.idle":"2025-09-08T14:23:05.511526Z","shell.execute_reply.started":"2025-09-08T13:43:46.375573Z","shell.execute_reply":"2025-09-08T14:23:05.510606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# new_df['llm_judge_verdict']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T14:27:05.678386Z","iopub.execute_input":"2025-09-08T14:27:05.678682Z","iopub.status.idle":"2025-09-08T14:27:05.685022Z","shell.execute_reply.started":"2025-09-08T14:27:05.678659Z","shell.execute_reply":"2025-09-08T14:27:05.684254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# new_df[['llm_judge_verdict']].to_csv('llm_judge_feature_test_individual_.csv', index=False)\n# print(\"\\nLLM Judge feature generation complete. Results saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T14:26:22.901851Z","iopub.execute_input":"2025-09-08T14:26:22.902533Z","iopub.status.idle":"2025-09-08T14:26:22.909137Z","shell.execute_reply.started":"2025-09-08T14:26:22.902511Z","shell.execute_reply":"2025-09-08T14:26:22.908617Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Preplexity Score**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nmodel_id = 'gpt2'\nmodel = GPT2LMHeadModel.from_pretrained(model_id)\ntokenizer = GPT2Tokenizer.from_pretrained(model_id)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\ndef calculate_perplexity(text, model, tokenizer):\n    \"\"\"\n    Calculates the perplexity of a given text using a GPT-2 model.\n    Lower perplexity means the text is more predictable and natural.\n    \"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return np.nan\n\n    inputs = tokenizer(text, \n                       truncation=True,       \n                       max_length=512,\n                       return_tensors=\"pt\").to(device)\n    input_ids = inputs.input_ids\n\n    with torch.no_grad():\n        outputs = model(**inputs, labels=input_ids)\n        neg_log_likelihood = outputs.loss\n        perplexity = torch.exp(neg_log_likelihood)\n        \n    return perplexity.item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T10:32:04.478441Z","iopub.execute_input":"2025-09-06T10:32:04.478729Z","iopub.status.idle":"2025-09-06T10:32:05.441000Z","shell.execute_reply.started":"2025-09-06T10:32:04.478709Z","shell.execute_reply":"2025-09-06T10:32:05.440396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ntrain_data_path = '/kaggle/input/fake-or-real-the-imposter-x-train/train_dataset.csv'\ntest_data_path = '/kaggle/input/fake-or-real-the-imposter-x-train/test_data.csv'\ndf = pd.read_csv(train_data_path)\ndf['text_1'] = df['text_1'].fillna('')\ndf['text_2'] = df['text_2'].fillna('')\nnew_df = pd.DataFrame()\nnew_df['text'] = pd.concat([df['text_1'],df['text_2']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"perplexity = []\nfor _,row in new_df.iterrows():\n    perpex = calculate_perplexity(row['text'],model,tokenizer)\n    perplexity.append(perpex)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T10:27:19.860061Z","iopub.execute_input":"2025-09-06T10:27:19.860334Z","iopub.status.idle":"2025-09-06T10:27:26.276870Z","shell.execute_reply.started":"2025-09-06T10:27:19.860309Z","shell.execute_reply":"2025-09-06T10:27:26.276306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df['perplexity_score'] = perplexity\nnew_df[['perplexity_score']].to_csv('perplexity_score_features_train.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T10:29:10.820812Z","iopub.execute_input":"2025-09-06T10:29:10.821407Z","iopub.status.idle":"2025-09-06T10:29:10.836006Z","shell.execute_reply.started":"2025-09-06T10:29:10.821383Z","shell.execute_reply":"2025-09-06T10:29:10.835367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ndf = pd.read_csv(test_data_path)\ndf['text_1'] = df['text_1'].fillna('')\ndf['text_2'] = df['text_2'].fillna('')\nnew_df = pd.DataFrame()\nnew_df['text'] = pd.concat([df['text_1'],df['text_2']])\nperplexity = []\nfor _,row in new_df.iterrows():\n    perpex = calculate_perplexity(row['text'],model,tokenizer)\n    perplexity.append(perpex)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T10:32:14.311065Z","iopub.execute_input":"2025-09-06T10:32:14.311346Z","iopub.status.idle":"2025-09-06T10:33:23.802754Z","shell.execute_reply.started":"2025-09-06T10:32:14.311325Z","shell.execute_reply":"2025-09-06T10:33:23.802201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df['perplexity_score'] = perplexity\nnew_df[['perplexity_score']].to_csv('perplexity_score_features_test.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T10:33:55.419131Z","iopub.execute_input":"2025-09-06T10:33:55.419797Z","iopub.status.idle":"2025-09-06T10:33:55.427572Z","shell.execute_reply.started":"2025-09-06T10:33:55.419772Z","shell.execute_reply":"2025-09-06T10:33:55.426792Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Sentence-Coherence Feature**","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nimport numpy as np\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept nltk.downloader.DownloadError:\n    nltk.download('punkt')\n\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ndef get_coherence_scores(text,max_sentences=50):\n    sentences = sent_tokenize(text)\n    sentences = sentences[:max_sentences]\n    if len(sentences) < 2:\n        return 0.0, 0.0 \n    sentence_embeddings = model.encode(sentences)\n\n\n    sequential_sims = [cosine_similarity([sentence_embeddings[i]], [sentence_embeddings[i+1]])[0][0] for i in range(len(sentences)-1)]\n    flow_coherence = np.mean(sequential_sims)\n\n\n    centroid = np.mean(sentence_embeddings, axis=0)\n    focus_sims = [cosine_similarity([embedding], [centroid])[0][0] for embedding in sentence_embeddings]\n    semantic_focus = np.mean(focus_sims)\n    \n    return flow_coherence, semantic_focus","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T12:43:00.519922Z","iopub.execute_input":"2025-09-06T12:43:00.520665Z","iopub.status.idle":"2025-09-06T12:43:40.355104Z","shell.execute_reply.started":"2025-09-06T12:43:00.520637Z","shell.execute_reply":"2025-09-06T12:43:40.354491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ntrain_data_path = '/kaggle/input/fake-or-real-the-imposter-x-train/train_dataset.csv'\ntest_data_path = '/kaggle/input/fake-or-real-the-imposter-x-train/test_data.csv'\ndf = pd.read_csv(train_data_path)\ndf['text_1'] = df['text_1'].fillna('')\ndf['text_2'] = df['text_2'].fillna('')\nnew_df = pd.DataFrame()\nnew_df['text'] = pd.concat([df['text_1'],df['text_2']])\nflow_coherence = []\nsemantic_focus = []\nfor _,row in new_df.iterrows():\n    flow_coherence_ , semantic_focus_ = get_coherence_scores(row['text'])\n    flow_coherence.append(flow_coherence_)\n    semantic_focus.append(semantic_focus_)\n\nprint(len(flow_coherence),len(semantic_focus))\n\nnp.save('flow_coherence_train.npy',flow_coherence)\nnp.save('semantic_focus_train.npy',semantic_focus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T12:25:34.138237Z","iopub.execute_input":"2025-09-06T12:25:34.138859Z","iopub.status.idle":"2025-09-06T12:25:34.143221Z","shell.execute_reply.started":"2025-09-06T12:25:34.138837Z","shell.execute_reply":"2025-09-06T12:25:34.142543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ntrain_data_path = '/kaggle/input/fake-or-real-the-imposter-x-train/train_dataset.csv'\ntest_data_path = '/kaggle/input/fake-or-real-the-imposter-x-train/test_data.csv'\ndf = pd.read_csv(test_data_path)\ndf['text_1'] = df['text_1'].fillna('')\ndf['text_2'] = df['text_2'].fillna('')\nnew_df = pd.DataFrame()\nnew_df['text'] = pd.concat([df['text_1'],df['text_2']])\nflow_coherence = []\nsemantic_focus = []\nfor _,row in new_df.iterrows():\n    flow_coherence_ , semantic_focus_ = get_coherence_scores(row['text'])\n    flow_coherence.append(flow_coherence_)\n    semantic_focus.append(semantic_focus_)\n\nprint(len(flow_coherence),len(semantic_focus))\n\nnp.save('flow_coherence_test.npy',flow_coherence)\nnp.save('semantic_focus_test.npy',semantic_focus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T12:54:25.182938Z","iopub.execute_input":"2025-09-06T12:54:25.183971Z","iopub.status.idle":"2025-09-06T12:54:25.189149Z","shell.execute_reply.started":"2025-09-06T12:54:25.183942Z","shell.execute_reply":"2025-09-06T12:54:25.188425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}