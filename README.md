# Fake-or-Real-The-Impostor-Hunt-in-Texts-My-solution

### Contest Link: [Fake-or-Real-The-Impostor-Hunt-in-Texts](https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt)

## üìù TL;DR  
### Built a pipeline for data-scarce NLP (95 samples) ‚Üí Started with ML baselines ‚Üí  advanced to transformer ensembles with pseudo-labeling ‚Üí final DeBERTa-large  distilled from an ensemble, scoring **0.91493 (Top 13%)**.

### Leaderboard progression of approaches
![model_accuracy_table](https://github.com/narayanpdas/Fake-or-Real-The-Impostor-Hunt-in-Texts-My-solution/blob/6808ae215999d6a9ed2736d4b6e7296b8004a46c/Assests/Model_accuracy_table.png)


## Details:

### This notebook demonstrates how I engineered a solution for a highly data-scarce NLP problem, combining traditional ML baselines with transformer ensembles to reach a final leaderboard score of 0.91493, ranking in the top 13%.

## ‚ö†Ô∏è Important Note: 

- Dataset not included (per competition rules).  
- Update paths in `pandas.read_csv()` before running.  
- Use `notebooks/utility/utility.py` to correctly extract and load data.  

## üéØGOAL of the Contest: 

### The primary challenge was to distinguish between texts  from a limited training set of 95 samples and both texts in each sample - real (optimal for the recipient, as close as possible to the hidden original text) and fake (more or much more distant from the hidden original text) - have been significantly modified using LLMs.

## My Final Strategy:

### My final strategy was to fine-tune a *DeBERTa-large* model using a semi-supervised approach with pseudo-labels generated by a powerful 5-model transformer ensemble. This approach successfully navigated the data scarcity issue by leveraging the large, unlabeled test set, yielding a final leaderboard score of 0.91493.

#### This approach was built upon the incredible work of the Kaggle community, integrating and inspired from several key ideas, like the [e.g., READX model ensemble] from the notebook by [omidbaghchehsaraei](https://www.kaggle.com/omidbaghchehsaraei) and advanced feature engineering concepts from [metawave](https://www.kaggle.com/metawave). This notebook documents how these concepts were combined and extended to create the final solution.More on Insipiration is at the end of this notebook.


# üßë‚Äçüç≥ Strategies: 

### After initial experiments, a key insight emerged: training a "single-text scorer" model ,one that learns the absolute properties of a "real" vs. "fake" text and assigns each a score, consistently outperformed direct pairwise comparison methods. This was likely due to the small dataset size, as the scorer model's training data could be doubled and perfectly debiased.

## Summary of Approaches & Results:

### While I experimented with numerous techniques, here is a summary of the most effective strategies in my journey.

## --- Machine Learning Only Approaches ---

### - *BaseLine* (TF-IDF with logistic Regression): 
This was my baseline/ first approach just to see with a normal and simple solution how much can a model learn. The model achieved a score of **0.778** on the leaderboard.This confirmed that while lexical features provided a strong signal, a more sophisticated approach would be required to capture the semantic and structural nuances.

### -*Ensemble of Baselines*: 
The underlying Principles are the same as the baseline but it uses predictions from 5 best machine learning models and it turned out to be logistic regression model with different Inverse Regularization (C) values. The final score from this was **0.84** this was a really good score considering how simple it is to many different models that i discussed later. *This proved lexical features alone weren‚Äôt enough.*

## --- Deep-Learning Only Approach ---

### - *BERT*: 
This was the simplest model to start from , just used the tokenizer from the bert model and trained it. The result was a positive **0.83195** score. This guided to me to move towards transformers architectures as they seem to learn better patterns here then simple machine learning models.

### - *Deberta with Psuedo-Labelling*: 
After Bert the most logical approach for me was to go for a bigger model(although i did many things in between) i.e. Deberta. Here I with the help of BERT model i did psuedo labelling of the test data and selected the labels (119) with confidence was more than 90%. This model made a score of **0.863**.*Hence Pseudo-labels gave a big boost despite limited true training data.*

## --- Hybrid Approaches ---

### - üîç*Forensic Expert Model*: 
Here i did several data visualization and augmentation methods to create a rich feature set. The goal here was to squeeze as much information from the 95 samples as possible to. The Results were positive it had a score of **0.82** on the leaderboard, but not quite enough to beat the ensemble of Base Lines.Here i used 'gpt2' for perplexity, 'all-MiniLM-L6-v2' for Sentence-Coherence and 'Meta-Llama-3-8B-Instruct' for LLM Judge. This model had a final score of **0.82**

### - *Deberta with Baseline Ensemble (The 25-Day Champion)*: 
My best-performing solution for over three weeks was a hybrid blend. By taking a weighted average of the probabilities from the *DeBERTa-with-Pseudo-Labeling model* and *the Ensemble of Baselines*, I achieved a score of **0.88**. This "best of both worlds" approach proved to be a very powerful strategy.

## --- The Best Solution ---

### - *R_E_A_D_X Model*: 
The first breakthrough beyond 0.88 came from ensembling five diverse transformer architectures: **R**oBERTa, **E**LECTRA, **A**LBERT, **D**eBERTa, and **X**LNet. Training these models on the orginal 95 samples and averaging their probabilities yielded a score of **0.885**. 

### - üèÜ*The Champion*: 
The final winning solution involved a form of knowledge distillation. The powerful READX ensemble was used as a "teacher" to generate 200 high-confidence pseudo-labels from the test set. A single DeBERTa-large model was then Pre-trained (higher learning rate) on the original data first and then fine-tuned(Lower Learning Rate) on these high-quality pseudo-labels with the original data . This "student" model learned the distilled knowledge of the entire ensemble, achieving the final best score of **0.91493**.
## --- Promising but not optimal Approaches---

### *DeBERTa with Domain Adaptation* :
An experiment was conducted to pre-train DeBERTa on the large, unlabeled test dataset before fine-tuning. This yielded a similar result to the standard fine-tuning, likely because the powerful - model adapted to the domain very quickly on its own.

### *The Champion with Reinforcement Learning*:
From the prediction probabilitues i had from the champion model it was obvserved that the model had many close calls where it miss labels by a 0.001 mark, By some Analysis i concluded that this happens when the texts are very similar to each other or they look very natural.A final experiment involved a custom training loop where the model was "punished" for being uncertain on unlabeled test data (entropy minimization). While a fascinating idea, this resulted in a slight diminishing return with a score of **0.881**, likely requiring a larger dataset to be truly effective.
### The Below is the sole reason for trying out this RL approach with the *Chapmpion Model*. 

![confidence_distribution](https://github.com/narayanpdas/Fake-or-Real-The-Impostor-Hunt-in-Texts-My-solution/blob/42fbbefb9623f244f3115ee20cecbdc29eac8b6a/Assests/deberta_confidence_plot.png)

*The following chart highlights how close the model‚Äôs predictions were, with most lying in a narrow band near decision boundaries.*

### As you can see the model had close calls for many datapoints (about 130 with <10% confidence and about 70 with <1% gap) making it predict certain datapoints by a small margin. This is about 10% of the test_set of 1000+ samples.

# Learnings 

### There is a lot that i learned in this journey, in the beginning i was able to build ML models but this Competition inspired me to learn a lot of NLP. From Various Data Augmentation techniques to finetuning a large model , i Learned a lot, and here are the few key things i learned:

- Baselines Matter: Simple models like TF-IDF + Logistic Regression gave surprisingly strong starting points.
- Scarcity Hacks: With only 95 samples, creative techniques (pseudo-labelling, augmentation) were essential.
- Pseudo-Labels : Using confident test predictions as training data boosted performance significantly.
- Data-spliting : Learned not only about train-test-split but also about groupkfolds, strategies in splitting data with key values.
- Power of Ensembling /Stacking : Combining multiple models captured diverse patterns better than any single model, also may a times stacking models seems to work better than ensembles.
- Hybrid Wins : Blending ML features with transformer predictions outperformed either alone.
- Progressive Scaling : Moving from small (BERT) to larger (DeBERTa-Large) models gave incremental gains.
- Confidence Filtering : Selecting only high-confidence pseudo-labels avoided noise and improved stability.
- Custom Loss Insight : Tried reinforcement-style loss adjustments‚Äîshowed potential but limited by data.
- Interpretability Attempts : Features like perplexity and coherence added human-style checks, though not always better than pure models.

- Iteration Counts : Improvement came step by step; no single ‚Äúmagic trick,‚Äù but layering ideas carefully.
### This Competition was an intense sprint for me, completed in just under 4 weeks after joining the competition late. While there are always more avenues to explore, I am satisfied of the robust and high-performing pipeline that I developed in this limited timeframe and looking forward to applying these learnings to future challenges and projects.

# Acknowledgements & Inspirations

### This journey would not have been possible without the amazing Kaggle community. I learned a great deal from public notebooks, studying how other data scientists approach and solve complex problems with clever and effective techniques. My sincere thanks to the entire community and to the organizers for hosting this fantastic competition.
### In particular, I'd like to acknowledge the following inspirations:
- My perplexity and LLM judge idea is inspired from [This Notebook](https://www.kaggle.com/code/metawave/memory-optimized-transformers-for-impostor-hunt)
- MY READX model was an direct inspiration from [This Notebook](https://www.kaggle.com/code/omidbaghchehsaraei/averaging-ensembling-for-the-impostor-hunt-in-text/notebook)

### This competition showed me that even with scarce data, creative ensembling and persistence can push boundaries and I look forward to applying these lessons to future challenges and Projects.